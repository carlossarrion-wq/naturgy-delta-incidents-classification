# Analizador Sem√°ntico de Incidencias T√©cnicas - Versi√≥n Refactorizada
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter, defaultdict
from typing import List, Dict, Optional
import re
import json
import os
from pathlib import Path
from datetime import datetime

class SemanticOutputManager:
    """Gestiona la estructura de salidas para el analizador sem√°ntico"""
    
    def __init__(self, base_output_dir: str = "outputs_semantic"):
        self.base_dir = Path(base_output_dir)
        self.setup_directory_structure()
    
    def setup_directory_structure(self):
        """Crea la estructura de carpetas necesaria"""
        directories = {
            'analysis': self.base_dir / 'analysis',
            'categories': self.base_dir / 'categories',
            'reports': self.base_dir / 'reports',
            'data': self.base_dir / 'data'
        }
        
        for dir_name, dir_path in directories.items():
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"üìÅ Directorio {dir_name} preparado: {dir_path}")
    
    def get_path(self, category: str, filename: str) -> Path:
        """Obtiene la ruta completa para un archivo seg√∫n su categor√≠a"""
        return self.base_dir / category / filename
    
    def save_categories(self, categories_data: dict, filename: str = 'categorias_semanticas.json'):
        """Guarda las categor√≠as en la carpeta categories"""
        filepath = self.get_path('categories', filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(categories_data, f, ensure_ascii=False, indent=2)
        print(f"üìä Categor√≠as guardadas: {filepath}")
        return filepath
    
    def save_analysis(self, analysis_data: dict, filename: str = 'analisis_completo.json'):
        """Guarda el an√°lisis completo en la carpeta analysis"""
        filepath = self.get_path('analysis', filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)
        print(f"üîç An√°lisis guardado: {filepath}")
        return filepath
    
    def save_report(self, content: str, filename: str):
        """Guarda reporte de texto en la carpeta reports"""
        filepath = self.get_path('reports', filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"üìÑ Reporte guardado: {filepath}")
        return filepath

class SemanticCategoryNaming:
    """Motor de nomenclatura sem√°ntica mejorada"""
    
    def __init__(self):
        self.setup_semantic_patterns()
    
    def setup_semantic_patterns(self):
        """Configura patrones sem√°nticos para nomenclatura con variaciones expandidas"""
        
        # Funci√≥n auxiliar para expandir keywords con variaciones
        def expand_keywords(base_keywords):
            expanded = set(base_keywords)  # Incluir las originales
            
            for keyword in base_keywords:
                # Agregar variaciones comunes
                variations = self._generate_keyword_variations(keyword)
                expanded.update(variations)
            
            return list(expanded)
        
        # Patrones de dominio funcional con keywords expandidas
        base_patterns = {
            'gestion_datos': {
                'keywords': ['datos', 'data', 'carga', 'actualizaci√≥n', 'masiva', 'bbdd', 'base'],
                'causes': ['actualizaci√≥n masiva de datos', 'carga de datos', 'sincronizaci√≥n'],
                'base_name': 'Gesti√≥n de Datos'
            },
            'infraestructura_sistema': {
                'keywords': ['infraestructura', 'sistema', 'servidor', 'red', 'conexi√≥n', 'servicio'],
                'causes': ['infraestructura', 'problema de conectividad', 'fallo del sistema'],
                'base_name': 'Infraestructura y Sistemas'
            },
            'comunicacion_notificacion': {
                'keywords': ['comunicaci√≥n', 'notificaci√≥n', 'env√≠o', 'correo', 'sms', 'alerta'],
                'causes': ['comunicaci√≥n', 'notificaci√≥n', 'env√≠o autom√°tico'],
                'base_name': 'Comunicaciones y Notificaciones'
            },
            'procesos_automaticos': {
                'keywords': ['batch', 'proceso', 'job', 'autom√°tico', 'programado', 'ejecuci√≥n'],
                'causes': ['proceso batch', 'job autom√°tico', 'ejecuci√≥n programada'],
                'base_name': 'Procesos Autom√°ticos'
            },
            'consultas_informacion': {
                'keywords': ['consulta', 'informaci√≥n', 'b√∫squeda', 'listado', 'extracci√≥n'],
                'causes': ['consulta funcional', 'solicitud de informaci√≥n', 'extracci√≥n de datos'],
                'base_name': 'Consultas e Informaci√≥n'
            },
            'gestion_contratos': {
                'keywords': ['contrato', 'alta', 'baja', 'modificaci√≥n', 'cambio', 'cliente'],
                'causes': ['gesti√≥n de contratos', 'alta de contrato', 'baja de contrato'],
                'base_name': 'Gesti√≥n de Contratos'
            },
            'facturacion_cobros': {
                'keywords': ['facturaci√≥n', 'factura', 'facturas', 'cobro', 'pago', 'importe', 'recibo'],
                'causes': ['facturaci√≥n', 'cobro', 'gesti√≥n econ√≥mica'],
                'base_name': 'Facturaci√≥n y Cobros'
            },
            'errores_tecnicos': {
                'keywords': ['error', 'fallo', 'excepci√≥n', 'timeout', 'crash', 'bug'],
                'causes': ['error t√©cnico', 'fallo del sistema', 'excepci√≥n'],
                'base_name': 'Errores T√©cnicos'
            }
        }
        
        # Expandir keywords para cada patr√≥n
        self.functional_patterns = {}
        for pattern_name, pattern_info in base_patterns.items():
            expanded_pattern = pattern_info.copy()
            expanded_pattern['keywords'] = expand_keywords(pattern_info['keywords'])
            self.functional_patterns[pattern_name] = expanded_pattern
    
    def _generate_keyword_variations(self, keyword):
        """Genera variaciones autom√°ticas de keywords (plurales, tildes, sin√≥nimos, etc.)"""
        variations = set()
        keyword_lower = keyword.lower()
        
        # 1. PLURALES AUTOM√ÅTICOS
        variations.update(self._generate_plurals(keyword_lower))
        
        # 2. VARIACIONES DE TILDES/ACENTOS
        variations.update(self._generate_accent_variations(keyword_lower))
        
        # 3. VARIACIONES DE G√âNERO
        variations.update(self._generate_gender_variations(keyword_lower))
        
        # 4. FORMAS VERBALES RELACIONADAS
        variations.update(self._generate_verb_forms(keyword_lower))
        
        # 5. SIN√ìNIMOS T√âCNICOS ESPEC√çFICOS
        variations.update(self._generate_technical_synonyms(keyword_lower))
        
        # 6. VARIACIONES CON PREFIJOS/SUFIJOS COMUNES
        variations.update(self._generate_prefix_suffix_variations(keyword_lower))
        
        return variations
    
    def _generate_plurals(self, word):
        """Genera plurales autom√°ticos"""
        plurals = set()
        
        # Reglas b√°sicas de pluralizaci√≥n en espa√±ol
        if word.endswith('a') or word.endswith('e') or word.endswith('i') or word.endswith('o') or word.endswith('u'):
            plurals.add(word + 's')
        elif word.endswith('z'):
            plurals.add(word[:-1] + 'ces')
        elif word.endswith('ci√≥n'):
            plurals.add(word[:-4] + 'ciones')
        elif word.endswith('si√≥n'):
            plurals.add(word[:-4] + 'siones')
        elif word.endswith('√≥n'):
            plurals.add(word[:-2] + 'ones')
        else:
            plurals.add(word + 'es')
        
        # Plurales en ingl√©s para t√©rminos t√©cnicos
        if word.endswith('y') and len(word) > 2 and word[-2] not in 'aeiou':
            plurals.add(word[:-1] + 'ies')
        elif word.endswith(('s', 'sh', 'ch', 'x', 'z')):
            plurals.add(word + 'es')
        elif word.endswith('f'):
            plurals.add(word[:-1] + 'ves')
        elif word.endswith('fe'):
            plurals.add(word[:-2] + 'ves')
        else:
            plurals.add(word + 's')  # Plural ingl√©s est√°ndar
        
        return plurals
    
    def _generate_accent_variations(self, word):
        """Genera variaciones con y sin tildes"""
        variations = set()
        
        # Mapeo de caracteres con tilde a sin tilde y viceversa
        accent_map = {
            '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u', '√±': 'n',
            'a': '√°', 'e': '√©', 'i': '√≠', 'o': '√≥', 'u': '√∫', 'n': '√±'
        }
        
        # Generar variaciones reemplazando caracteres
        for i, char in enumerate(word):
            if char in accent_map:
                variation = word[:i] + accent_map[char] + word[i+1:]
                variations.add(variation)
        
        # Casos espec√≠ficos comunes
        specific_variations = {
            'informacion': 'informaci√≥n',
            'informaci√≥n': 'informacion',
            'actualizacion': 'actualizaci√≥n',
            'actualizaci√≥n': 'actualizacion',
            'comunicacion': 'comunicaci√≥n',
            'comunicaci√≥n': 'comunicacion',
            'notificacion': 'notificaci√≥n',
            'notificaci√≥n': 'notificacion',
            'facturacion': 'facturaci√≥n',
            'facturaci√≥n': 'facturacion',
            'modificacion': 'modificaci√≥n',
            'modificaci√≥n': 'modificacion',
            'excepcion': 'excepci√≥n',
            'excepci√≥n': 'excepcion',
            'ejecucion': 'ejecuci√≥n',
            'ejecuci√≥n': 'ejecucion',
            'conexion': 'conexi√≥n',
            'conexi√≥n': 'conexion'
        }
        
        if word in specific_variations:
            variations.add(specific_variations[word])
        
        return variations
    
    def _generate_gender_variations(self, word):
        """Genera variaciones de g√©nero"""
        variations = set()
        
        # Cambios de g√©nero comunes
        if word.endswith('o'):
            variations.add(word[:-1] + 'a')
        elif word.endswith('a'):
            variations.add(word[:-1] + 'o')
        elif word.endswith('or'):
            variations.add(word + 'a')
        elif word.endswith('ora'):
            variations.add(word[:-1])
        
        return variations
    
    def _generate_verb_forms(self, word):
        """Genera formas verbales relacionadas"""
        variations = set()
        
        # Mapeo de sustantivos a formas verbales comunes
        verb_forms = {
            'carga': ['cargar', 'cargando', 'cargado', 'cargas'],
            'actualizaci√≥n': ['actualizar', 'actualizando', 'actualizado', 'actualiza'],
            'modificaci√≥n': ['modificar', 'modificando', 'modificado', 'modifica'],
            'env√≠o': ['enviar', 'enviando', 'enviado', 'env√≠a'],
            'b√∫squeda': ['buscar', 'buscando', 'buscado', 'busca'],
            'extracci√≥n': ['extraer', 'extrayendo', 'extra√≠do', 'extrae'],
            'ejecuci√≥n': ['ejecutar', 'ejecutando', 'ejecutado', 'ejecuta'],
            'conexi√≥n': ['conectar', 'conectando', 'conectado', 'conecta'],
            'facturaci√≥n': ['facturar', 'facturando', 'facturado', 'factura'],
            'error': ['errar', 'errando', 'errado', 'erra'],
            'fallo': ['fallar', 'fallando', 'fallado', 'falla'],
            'proceso': ['procesar', 'procesando', 'procesado', 'procesa'],
            'consulta': ['consultar', 'consultando', 'consultado', 'consulta'],
            'cambio': ['cambiar', 'cambiando', 'cambiado', 'cambia'],
            'pago': ['pagar', 'pagando', 'pagado', 'paga']
        }
        
        if word in verb_forms:
            variations.update(verb_forms[word])
        
        return variations
    
    def _generate_technical_synonyms(self, word):
        """Genera sin√≥nimos t√©cnicos espec√≠ficos"""
        variations = set()
        
        # Diccionario de sin√≥nimos t√©cnicos
        synonyms = {
            'datos': ['data', 'informaci√≥n', 'info', 'registros'],
            'sistema': ['aplicaci√≥n', 'aplicacion', 'app', 'plataforma'],
            'servidor': ['server', 'host', 'm√°quina', 'maquina'],
            'error': ['fallo', 'excepci√≥n', 'excepcion', 'bug', 'incidencia'],
            'proceso': ['job', 'tarea', 'procedimiento'],
            'autom√°tico': ['automatico', 'auto', 'programado'],
            'b√∫squeda': ['busqueda', 'query', 'consulta'],
            'correo': ['email', 'mail', 'mensaje'],
            'contrato': ['acuerdo', 'convenio'],
            'factura': ['recibo', 'comprobante'],
            'cliente': ['usuario', 'consumidor'],
            'alta': ['creaci√≥n', 'creacion', 'registro'],
            'baja': ['eliminaci√≥n', 'eliminacion', 'cancelaci√≥n', 'cancelacion'],
            'conexi√≥n': ['conexion', 'conectividad', 'enlace'],
            'servicio': ['service', 'funci√≥n', 'funcion'],
            'base': ['bd', 'database', 'repositorio'],
            'bbdd': ['base de datos', 'database', 'bd'],
            'batch': ['lote', 'masivo', 'job'],
            'timeout': ['tiempo agotado', 'expiraci√≥n', 'expiracion'],
            'crash': ['ca√≠da', 'caida', 'colapso']
        }
        
        if word in synonyms:
            variations.update(synonyms[word])
        
        # Tambi√©n buscar si el word es sin√≥nimo de alguna clave
        for key, synonym_list in synonyms.items():
            if word in synonym_list:
                variations.add(key)
                variations.update(synonym_list)
        
        return variations
    
    def _generate_prefix_suffix_variations(self, word):
        """Genera variaciones con prefijos y sufijos comunes"""
        variations = set()
        
        # Prefijos comunes
        prefixes = ['auto', 'sub', 'pre', 'post', 're', 'des', 'co']
        
        # Sufijos comunes
        suffixes = ['ado', 'ido', 'ando', 'iendo', 'able', 'ible', 'mente']
        
        # Agregar prefijos
        for prefix in prefixes:
            if not word.startswith(prefix):
                variations.add(prefix + word)
        
        # Agregar sufijos (con cuidado de no duplicar)
        for suffix in suffixes:
            if not word.endswith(suffix):
                variations.add(word + suffix)
        
        # Variaciones espec√≠ficas conocidas
        specific_prefix_suffix = {
            'cargar': ['descargar', 'recargar', 'precargar'],
            'conectar': ['desconectar', 'reconectar'],
            'activar': ['desactivar', 'reactivar'],
            'hacer': ['deshacer', 'rehacer'],
            'instalar': ['desinstalar', 'reinstalar'],
            'configurar': ['reconfigurar', 'preconfigurar']
        }
        
        if word in specific_prefix_suffix:
            variations.update(specific_prefix_suffix[word])
        
        return variations
        
        # Modificadores de contexto
        self.context_modifiers = {
            'urgente': ['urgente', 'cr√≠tico', 'inmediato', 'prioritario'],
            'masivo': ['masivo', 'm√∫ltiple', 'lote', 'bulk', 'varios'],
            'nocturno': ['nocturno', 'madrugada', 'fuera de horario'],
            'especifico': ['espec√≠fico', 'puntual', 'individual', '√∫nico']
        }
    
    def generate_semantic_category_name(self, cluster_df: pd.DataFrame, 
                                      top_words: List[str], 
                                      top_causes: List[str]) -> str:
        """Genera nombre sem√°nticamente coherente y t√©cnicamente correcto"""
        
        # Combinar todo el texto para an√°lisis
        all_text = self._combine_cluster_text(cluster_df).lower()
        
        # Identificar patr√≥n funcional principal
        main_pattern = self._identify_functional_pattern(all_text, top_words, top_causes)
        
        # Identificar modificadores de contexto
        context_modifier = self._identify_context_modifier(all_text)
        
        # Generar nombre base
        if main_pattern:
            base_name = self.functional_patterns[main_pattern]['base_name']
        else:
            # Fallback basado en palabras clave
            base_name = self._generate_fallback_name(top_words, top_causes)
        
        # Aplicar modificador si existe
        if context_modifier:
            semantic_name = f"{base_name} {context_modifier.title()}"
        else:
            semantic_name = base_name
        
        # A√±adir especificidad basada en el tama√±o
        size_spec = self._get_size_specification(len(cluster_df))
        if size_spec:
            semantic_name = f"{semantic_name} - {size_spec}"
        
        return semantic_name
    
    def _combine_cluster_text(self, cluster_df: pd.DataFrame) -> str:
        """Combina todo el texto del cluster para an√°lisis"""
        text_parts = []
        
        for col in ['Resumen', 'Notas', 'Tipo de ticket']:
            if col in cluster_df.columns:
                text_parts.extend(cluster_df[col].fillna('').astype(str).tolist())
        
        return ' '.join(text_parts)
    
    def _identify_functional_pattern(self, text: str, words: List[str], 
                                   causes: List[str]) -> Optional[str]:
        """Identifica el patr√≥n funcional principal"""
        pattern_scores = {}
        
        for pattern_name, pattern_info in self.functional_patterns.items():
            score = 0
            
            # Puntuaci√≥n por keywords
            for keyword in pattern_info['keywords']:
                score += text.count(keyword.lower()) * 2
            
            # Puntuaci√≥n por palabras clave extra√≠das
            for word in words:
                if word.lower() in pattern_info['keywords']:
                    score += 3
            
            # Puntuaci√≥n por causas
            for cause in causes:
                for pattern_cause in pattern_info['causes']:
                    if pattern_cause.lower() in cause.lower():
                        score += 5
            
            pattern_scores[pattern_name] = score
        
        # Retornar el patr√≥n con mayor puntuaci√≥n si supera el umbral
        if pattern_scores:
            max_pattern = max(pattern_scores.items(), key=lambda x: x[1])
            return max_pattern[0] if max_pattern[1] > 2 else None
        
        return None
    
    def _identify_context_modifier(self, text: str) -> Optional[str]:
        """Identifica modificadores de contexto"""
        for modifier_name, modifier_keywords in self.context_modifiers.items():
            for keyword in modifier_keywords:
                if keyword.lower() in text:
                    return modifier_name
        return None
    
    def _generate_fallback_name(self, words: List[str], causes: List[str]) -> str:
        """Genera nombre alternativo basado en an√°lisis directo"""
        if causes and len(causes) > 0:
            # Usar la causa principal como base
            main_cause = causes[0]
            if len(main_cause) > 30:
                return main_cause[:30] + "..."
            return main_cause
        
        elif words and len(words) > 0:
            # Usar palabras clave m√°s relevantes
            relevant_words = [w for w in words[:3] if len(w) > 3]
            if len(relevant_words) >= 2:
                return f"Incidencias {relevant_words[0].title()} y {relevant_words[1].title()}"
            elif len(relevant_words) == 1:
                return f"Incidencias de {relevant_words[0].title()}"
        
        return "Categor√≠a T√©cnica Espec√≠fica"
    
    def _get_size_specification(self, size: int) -> Optional[str]:
        """Obtiene especificaci√≥n basada en el tama√±o del cluster"""
        if size > 100:
            return "Alta Frecuencia"
        elif size > 50:
            return "Frecuencia Media"
        elif size < 10:
            return "Casos Espec√≠ficos"
        return None

class SemanticIncidentAnalyzer:
    """
    Analiza y categoriza incidencias t√©cnicas con nomenclatura sem√°ntica mejorada.
    """
    
    def __init__(self, max_categories=20, output_dir="outputs_semantic"):
        self.max_categories = max_categories
        self.output_manager = SemanticOutputManager(output_dir)
        self.naming_engine = SemanticCategoryNaming()
        
        self.vectorizer = TfidfVectorizer(
            max_features=3000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2),
            stop_words=None
        )
        self.categories = {}
        self.incident_vectors = None
        
    def clean_text(self, text):
        """Limpia y normaliza el texto"""
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text).lower()
        # Mantener caracteres espa√±oles y t√©cnicos importantes
        text = re.sub(r'[^a-z√°√©√≠√≥√∫√±√º0-9\s\-_]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def extract_technical_keywords(self, text):
        """Extrae palabras clave t√©cnicas espec√≠ficas"""
        keywords = []
        
        # Patrones t√©cnicos comunes
        patterns = {
            'error_types': r'\b(fail|error|exception|timeout|crash|bug)\b',
            'systems': r'\b(delta|atlas|ppm|oracle|mysql|database|sql)\b',
            'operations': r'\b(batch|job|process|script|execution|run)\b',
            'network': r'\b(connection|network|ftp|http|api|service)\b',
            'data': r'\b(datos|data|carga|load|backup|restore|sync)\b',
            'infrastructure': r'\b(server|infraestructura|sistema|aplicacion|app)\b'
        }
        
        text_lower = text.lower()
        for category, pattern in patterns.items():
            matches = re.findall(pattern, text_lower)
            if matches:
                keywords.extend([f"{category}_{match}" for match in matches])
        
        return ' '.join(keywords)
    
    def prepare_text_for_analysis(self, df):
        """Prepara los textos combinados para an√°lisis"""
        combined_texts = []
        
        for idx, row in df.iterrows():
            # Combinar todos los campos de texto relevantes
            text_parts = []
            
            if not pd.isna(row.get('Resumen', '')):
                text_parts.append(self.clean_text(row['Resumen']))
            
            if not pd.isna(row.get('Notas', '')):
                text_parts.append(self.clean_text(row['Notas']))
            
            if not pd.isna(row.get('Causa Ra√≠z', '')):
                text_parts.append(self.clean_text(row['Causa Ra√≠z']))
            
            # Extraer keywords t√©cnicos
            full_text = ' '.join(text_parts)
            technical_keywords = self.extract_technical_keywords(full_text)
            
            # Combinar texto limpio con keywords t√©cnicos
            final_text = f"{full_text} {technical_keywords}"
            combined_texts.append(final_text)
        
        return combined_texts
    
    def analyze_incidents(self, df):
        """Analiza las incidencias y las agrupa en categor√≠as sem√°nticas mejoradas"""
        print("üîç Iniciando an√°lisis sem√°ntico avanzado de incidencias...")
        
        # Preparar textos
        texts = self.prepare_text_for_analysis(df)
        
        # Vectorizar textos
        try:
            X = self.vectorizer.fit_transform(texts)
            self.incident_vectors = X
            print(f"‚úÖ Vectorizaci√≥n completada: {X.shape}")
        except Exception as e:
            print(f"‚ùå Error en vectorizaci√≥n: {e}")
            return None
        
        # Clustering inicial con m√°s clusters para luego fusionar
        initial_clusters = min(30, len(df) // 20)  # Empezar con m√°s clusters
        print(f"üéØ Clustering inicial con {initial_clusters} clusters...")
        
        kmeans = KMeans(n_clusters=initial_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X)
        
        # Analizar clusters y crear categor√≠as preliminares
        preliminary_categories = self._analyze_clusters_improved(df, texts, cluster_labels, kmeans)
        
        # Fusionar categor√≠as similares hasta llegar a m√°ximo permitido
        final_categories = self._merge_similar_categories(preliminary_categories, df)
        
        # Asignar nombres y descripciones finales con nomenclatura mejorada
        self.categories = self._finalize_categories_improved(final_categories, df)
        
        print(f"‚úÖ An√°lisis completado: {len(self.categories)} categor√≠as identificadas")
        
        # Guardar resultados organizados
        self._save_organized_results(df)
        
        return self.categories
    
    def _analyze_clusters_improved(self, df, texts, cluster_labels, kmeans):
        """Analiza cada cluster con t√©cnicas mejoradas"""
        categories = {}
        
        for cluster_id in range(len(set(cluster_labels))):
            # Obtener √≠ndices del cluster
            cluster_indices = np.where(cluster_labels == cluster_id)[0]
            
            if len(cluster_indices) == 0:
                continue
            
            # Obtener textos del cluster
            cluster_texts = [texts[i] for i in cluster_indices]
            cluster_df = df.iloc[cluster_indices].copy()
            
            # Analizar palabras m√°s frecuentes con filtros mejorados
            cluster_text = ' '.join(cluster_texts)
            words = self._extract_meaningful_words(cluster_text)
            
            # Analizar causas ra√≠z m√°s comunes
            if 'Causa Ra√≠z' in cluster_df.columns:
                cause_counts = cluster_df['Causa Ra√≠z'].value_counts()
                top_causes = cause_counts.head(3).index.tolist()
            else:
                top_causes = []
            
            # Crear categor√≠a preliminar con informaci√≥n mejorada
            categories[cluster_id] = {
                'incidents': cluster_indices.tolist(),
                'top_words': words[:8],
                'top_causes': top_causes,
                'size': len(cluster_indices),
                'cluster_center': kmeans.cluster_centers_[cluster_id],
                'semantic_features': self._extract_semantic_features(cluster_df)
            }
        
        return categories
    
    def _extract_meaningful_words(self, text: str) -> List[str]:
        """Extrae palabras significativas con filtros avanzados"""
        words = text.split()
        
        # Stop words t√©cnicas expandidas
        stop_words = {
            'de', 'la', 'el', 'en', 'y', 'a', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su',
            'por', 'con', 'para', 'como', 'las', 'del', 'los', 'un', 'una', 'al',
            'que', 'fue', 'son', 'han', 'muy', 'm√°s', 'este', 'esta', 'ese', 'esa',
            'tiene', 'hacer', 'todo', 'a√±o', 'd√≠a', 'mes', 'vez', 'caso', 'forma', 'parte',
            'pero', 'sin', 'sobre', 'entre', 'hasta', 'donde', 'cuando', 'porque'
        }
        
        # Filtrar palabras significativas
        meaningful_words = []
        for word in words:
            if (len(word) > 3 and 
                word.lower() not in stop_words and
                not word.isdigit() and
                any(c.isalpha() for c in word)):
                meaningful_words.append(word.lower())
        
        # Contar frecuencias y retornar las m√°s importantes
        word_freq = Counter(meaningful_words)
        return [word for word, freq in word_freq.most_common(10) if freq > 1]
    
    def _extract_semantic_features(self, cluster_df: pd.DataFrame) -> Dict:
        """Extrae caracter√≠sticas sem√°nticas del cluster"""
        features = {
            'avg_text_length': 0,
            'has_technical_terms': False,
            'urgency_indicators': 0,
            'time_patterns': []
        }
        
        if 'Resumen' in cluster_df.columns:
            # Longitud promedio de texto
            text_lengths = cluster_df['Resumen'].fillna('').astype(str).apply(len)
            features['avg_text_length'] = text_lengths.mean()
            
            # T√©rminos t√©cnicos
            all_text = ' '.join(cluster_df['Resumen'].fillna('').astype(str)).lower()
            technical_terms = ['sistema', 'error', 'fallo', 'proceso', 'batch', 'datos']
            features['has_technical_terms'] = any(term in all_text for term in technical_terms)
            
            # Indicadores de urgencia
            urgency_terms = ['urgente', 'cr√≠tico', 'inmediato', 'bloqueo']
            features['urgency_indicators'] = sum(all_text.count(term) for term in urgency_terms)
        
        return features
    
    def _merge_similar_categories(self, categories, df):
        """Fusiona categor√≠as similares con criterios mejorados"""
        print(f"üîÑ Fusionando categor√≠as similares (inicial: {len(categories)}, objetivo: ‚â§{self.max_categories})")
        
        while len(categories) > self.max_categories:
            # Encontrar las dos categor√≠as m√°s similares
            max_similarity = -1
            merge_pair = None
            
            cat_ids = list(categories.keys())
            for i in range(len(cat_ids)):
                for j in range(i + 1, len(cat_ids)):
                    cat1, cat2 = cat_ids[i], cat_ids[j]
                    
                    # Calcular similitud mejorada
                    sim_score = self._calculate_enhanced_similarity(
                        categories[cat1], categories[cat2]
                    )
                    
                    if sim_score > max_similarity:
                        max_similarity = sim_score
                        merge_pair = (cat1, cat2)
            
            # Fusionar las categor√≠as m√°s similares
            if merge_pair and max_similarity > 0.1:  # Umbral m√≠nimo
                cat1, cat2 = merge_pair
                merged_category = self._merge_two_categories_improved(
                    categories[cat1], categories[cat2]
                )
                
                # Reemplazar con la categor√≠a fusionada
                new_id = min(cat1, cat2)
                categories[new_id] = merged_category
                del categories[max(cat1, cat2)]
                
                print(f"  Fusionadas categor√≠as {cat1} y {cat2} (similitud: {max_similarity:.3f})")
            else:
                # Si no hay suficiente similitud, parar la fusi√≥n
                break
        
        return categories
    
    def _calculate_enhanced_similarity(self, cat1, cat2):
        """Calcula similitud mejorada entre categor√≠as"""
        # Similitud de palabras clave (40%)
        words1 = set(cat1['top_words'])
        words2 = set(cat2['top_words'])
        word_sim = len(words1.intersection(words2)) / len(words1.union(words2)) if words1.union(words2) else 0
        
        # Similitud de causas ra√≠z (40%)
        causes1 = set(cat1['top_causes'])
        causes2 = set(cat2['top_causes'])
        cause_sim = len(causes1.intersection(causes2)) / len(causes1.union(causes2)) if causes1.union(causes2) else 0
        
        # Similitud de caracter√≠sticas sem√°nticas (20%)
        semantic_sim = 0
        if 'semantic_features' in cat1 and 'semantic_features' in cat2:
            feat1 = cat1['semantic_features']
            feat2 = cat2['semantic_features']
            
            # Comparar caracter√≠sticas booleanas
            if feat1['has_technical_terms'] == feat2['has_technical_terms']:
                semantic_sim += 0.5
            
            # Comparar indicadores de urgencia
            if abs(feat1['urgency_indicators'] - feat2['urgency_indicators']) <= 1:
                semantic_sim += 0.5
        
        # Combinar similitudes con pesos
        total_sim = 0.4 * word_sim + 0.4 * cause_sim + 0.2 * semantic_sim
        return total_sim
    
    def _merge_two_categories_improved(self, cat1, cat2):
        """Fusiona dos categor√≠as con informaci√≥n mejorada"""
        merged = {
            'incidents': cat1['incidents'] + cat2['incidents'],
            'size': cat1['size'] + cat2['size']
        }
        
        # Combinar palabras clave (mantener las m√°s frecuentes)
        all_words = cat1['top_words'] + cat2['top_words']
        word_counts = Counter(all_words)
        merged['top_words'] = [word for word, count in word_counts.most_common(8)]
        
        # Combinar causas ra√≠z
        all_causes = cat1['top_causes'] + cat2['top_causes']
        cause_counts = Counter(all_causes)
        merged['top_causes'] = [cause for cause, count in cause_counts.most_common(3)]
        
        # Promedio de centros de cluster
        if 'cluster_center' in cat1 and 'cluster_center' in cat2:
            merged['cluster_center'] = (cat1['cluster_center'] + cat2['cluster_center']) / 2
        
        # Combinar caracter√≠sticas sem√°nticas
        if 'semantic_features' in cat1 and 'semantic_features' in cat2:
            merged['semantic_features'] = {
                'avg_text_length': (cat1['semantic_features']['avg_text_length'] + 
                                  cat2['semantic_features']['avg_text_length']) / 2,
                'has_technical_terms': (cat1['semantic_features']['has_technical_terms'] or 
                                      cat2['semantic_features']['has_technical_terms']),
                'urgency_indicators': (cat1['semantic_features']['urgency_indicators'] + 
                                     cat2['semantic_features']['urgency_indicators'])
            }
        
        return merged
    
    def _finalize_categories_improved(self, categories, df):
        """Asigna nombres y descripciones finales con nomenclatura sem√°ntica mejorada"""
        final_categories = {}
        
        for i, (cat_id, category) in enumerate(categories.items()):
            # Generar nombre sem√°nticamente coherente
            semantic_name = self.naming_engine.generate_semantic_category_name(
                df.iloc[category['incidents']], 
                category['top_words'], 
                category['top_causes']
            )
            
            # Generar descripci√≥n t√©cnica detallada
            description = self._generate_enhanced_description(category, df, semantic_name)
            
            # Obtener ejemplos representativos
            examples = self._get_enhanced_examples(category, df)
            
            # Evaluar criticidad y prioridad
            criticality = self._assess_category_criticality(category, df)
            
            # Crear clave sem√°nticamente significativa
            safe_key = re.sub(r'[^\w\s-]', '', semantic_name.lower())
            safe_key = re.sub(r'[\s-]+', '_', safe_key)
            
            final_categories[safe_key] = {
                'nombre': semantic_name,
                'descripcion': description,
                'ejemplos': examples,
                'num_incidencias': category['size'],
                'incident_ids': category['incidents'][:10],
                'palabras_clave': category['top_words'],
                'causas_principales': category['top_causes'],
                'criticidad': criticality,
                'caracteristicas_tecnicas': category.get('semantic_features', {}),
                'porcentaje_total': round((category['size'] / len(df)) * 100, 2)
            }
        
        return final_categories
    
    def _generate_enhanced_description(self, category, df, semantic_name):
        """Genera descripci√≥n t√©cnica mejorada"""
        size = category['size']
        causes = category['top_causes']
        words = category['top_words']
        
        description = f"Categor√≠a '{semantic_name}' que engloba {size} incidencias t√©cnicas "
        
        # Descripci√≥n basada en caracter√≠sticas sem√°nticas
        if 'semantic_features' in category:
            features = category['semantic_features']
            if features['has_technical_terms']:
                description += "de naturaleza t√©cnica especializada, "
            if features['urgency_indicators'] > 0:
                description += "que requieren atenci√≥n prioritaria, "
        
        # Contexto funcional
        if causes:
            main_cause = causes[0]
            description += f"principalmente relacionadas con '{main_cause}'. "
            
            if len(causes) > 1:
                description += f"Causas secundarias incluyen: {', '.join(causes[1:2])}. "
        
        # Patrones t√©cnicos identificados
        tech_words = [w for w in words if len(w) > 4]
        if tech_words:
            description += f"T√©rminos t√©cnicos frecuentes: {', '.join(tech_words[:4])}."
        
        return description
    
    def _get_enhanced_examples(self, category, df):
        """Obtiene ejemplos representativos mejorados"""
        incidents = category['incidents'][:5]
        examples = []
        
        for idx in incidents:
            if idx < len(df):
                row = df.iloc[idx]
                example = {
                    'id': row.get('Ticket ID', f'idx_{idx}'),
                    'resumen': str(row.get('Resumen', ''))[:120] + '...' if len(str(row.get('Resumen', ''))) > 120 else str(row.get('Resumen', '')),
                    'causa_raiz': str(row.get('Causa Ra√≠z', '')),
                    'prioridad': row.get('Prioridad', 'No especificada')
                }
                examples.append(example)
        
        return examples
    
    #Sirve para asignar una criticidad a la incidencia que hay que abordar

    def _assess_category_criticality(self, category, df):
        """Eval√∫a la criticidad de la categor√≠a"""
        if 'semantic_features' not in category:
            return 'Media'
        
        features = category['semantic_features']
        urgency_score = features.get('urgency_indicators', 0)
        
        if urgency_score > 5:
            return 'Alta'
        elif urgency_score > 2:
            return 'Media'
        else:
            return 'Baja'
    
    def _save_organized_results(self, df):
        """Guarda los resultados con estructura organizada"""
        
        # 1. Guardar categor√≠as con metadatos completos
        categories_data = {
            'metadata': {
                'fecha_analisis': datetime.now().isoformat(),
                'total_incidencias_analizadas': len(df),
                'total_categorias': len(self.categories),
                'metodo_analisis': 'An√°lisis Sem√°ntico Avanzado',
                'max_categorias_permitidas': self.max_categories
            },
            'categorias': self.categories
        }
        
        self.output_manager.save_categories(categories_data)
        
        # 2. Guardar an√°lisis detallado
        analysis_data = {
            'resumen_ejecutivo': self._generate_executive_summary(df),
            'estadisticas_clustering': self._get_clustering_stats(df),
            'distribuciones': self._get_category_distributions(),
            'recomendaciones': self._generate_recommendations()
        }
        
        self.output_manager.save_analysis(analysis_data)
        
        # 3. Generar y guardar reportes
        comprehensive_report = self.generate_comprehensive_report()
        self.output_manager.save_report(comprehensive_report, 'reporte_semantico_completo.txt')
        
        technical_summary = self._generate_technical_summary()
        self.output_manager.save_report(technical_summary, 'resumen_tecnico.txt')
    
    def _generate_executive_summary(self, df):
        """Genera resumen ejecutivo del an√°lisis"""
        total_categories = len(self.categories)
        high_criticality = sum(1 for cat in self.categories.values() if cat.get('criticidad') == 'Alta')
        
        return {
            'total_incidencias': len(df),
            'categorias_identificadas': total_categories,
            'categorias_alta_criticidad': high_criticality,
            'cobertura_analisis': f"{(total_categories / self.max_categories) * 100:.1f}%",
            'categoria_mas_frecuente': max(self.categories.items(), key=lambda x: x[1]['num_incidencias'])[1]['nombre'] if self.categories else 'N/A'
        }
    
    def _get_clustering_stats(self, df):
        """Obtiene estad√≠sticas del clustering"""
        if not self.categories:
            return {}
        
        sizes = [cat['num_incidencias'] for cat in self.categories.values()]
        return {
            'tamano_promedio_categoria': np.mean(sizes),
            'tamano_mediano_categoria': np.median(sizes),
            'categoria_mas_grande': max(sizes),
            'categoria_mas_pequena': min(sizes),
            'desviacion_estandar': np.std(sizes)
        }
    
    def _get_category_distributions(self):
        """Obtiene distribuciones de las categor√≠as"""
        if not self.categories:
            return {}
        
        # Distribuci√≥n por criticidad
        criticality_dist = {}
        for cat in self.categories.values():
            crit = cat.get('criticidad', 'No evaluada')
            criticality_dist[crit] = criticality_dist.get(crit, 0) + 1
        
        # Distribuci√≥n por tama√±o
        size_dist = {'Peque√±as (<20)': 0, 'Medianas (20-50)': 0, 'Grandes (>50)': 0}
        for cat in self.categories.values():
            size = cat['num_incidencias']
            if size < 20:
                size_dist['Peque√±as (<20)'] += 1
            elif size <= 50:
                size_dist['Medianas (20-50)'] += 1
            else:
                size_dist['Grandes (>50)'] += 1
        
        return {
            'por_criticidad': criticality_dist,
            'por_tamano': size_dist
        }
    
    def _generate_recommendations(self):
        """Genera recomendaciones basadas en el an√°lisis"""
        recommendations = []
        
        if not self.categories:
            return ['No se pudieron generar recomendaciones - No hay categor√≠as disponibles']
        
        # Recomendaciones basadas en criticidad
        high_crit_cats = [cat for cat in self.categories.values() if cat.get('criticidad') == 'Alta']
        if high_crit_cats:
            recommendations.append(f"Priorizar atenci√≥n a {len(high_crit_cats)} categor√≠as de alta criticidad")
        
        # Recomendaciones basadas en volumen
        large_cats = [cat for cat in self.categories.values() if cat['num_incidencias'] > 50]
        if large_cats:
            recommendations.append(f"Implementar automatizaci√≥n en {len(large_cats)} categor√≠as de alto volumen")
        
        # Recomendaciones generales
        recommendations.extend([
            "Desarrollar procedimientos espec√≠ficos por categor√≠a identificada",
            "Establecer KPIs de seguimiento por tipo de incidencia",
            "Revisar y actualizar el an√°lisis peri√≥dicamente"
        ])
        
        return recommendations
    
    def generate_comprehensive_report(self):
        """Genera reporte completo del an√°lisis sem√°ntico"""
        if not self.categories:
            return "No hay categor√≠as disponibles para generar el reporte."
        
        report = []
        report.append("=" * 80)
        report.append("üìä AN√ÅLISIS SEM√ÅNTICO AVANZADO DE INCIDENCIAS T√âCNICAS")
        report.append("=" * 80)
        report.append(f"Fecha de an√°lisis: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Total de categor√≠as identificadas: {len(self.categories)}")
        report.append(f"M√°ximo permitido: {self.max_categories}")
        report.append("")
        
        # Ordenar categor√≠as por n√∫mero de incidencias (descendente)
        sorted_categories = sorted(
            self.categories.items(), 
            key=lambda x: x[1]['num_incidencias'], 
            reverse=True
        )
        
        report.append("üè∑Ô∏è  CATEGOR√çAS SEM√ÅNTICAMENTE IDENTIFICADAS")
        report.append("=" * 80)
        
        for cat_key, category in sorted_categories:
            report.append(f"\nüìã {category['nombre'].upper()}")
            report.append("-" * 60)
            report.append(f"üìä Incidencias: {category['num_incidencias']} ({category.get('porcentaje_total', 0)}% del total)")
            report.append(f"‚ö° Criticidad: {category.get('criticidad', 'No evaluada')}")
            report.append(f"üìù Descripci√≥n: {category['descripcion']}")
            report.append(f"üîë Palabras clave: {', '.join(category['palabras_clave'])}")
            report.append(f"‚ö° Causas principales: {', '.join(category['causas_principales'])}")
            
            # Caracter√≠sticas t√©cnicas
            if 'caracteristicas_tecnicas' in category:
                features = category['caracteristicas_tecnicas']
                if features.get('has_technical_terms'):
                    report.append("üîß Naturaleza: T√©cnica especializada")
                if features.get('urgency_indicators', 0) > 0:
                    report.append(f"‚ö†Ô∏è  Indicadores de urgencia: {features['urgency_indicators']}")
            
            report.append("")
            report.append("üìã Ejemplos representativos:")
            
            for i, example in enumerate(category['ejemplos'][:3], 1):
                report.append(f"  {i}. ID: {example['id']}")
                report.append(f"     Resumen: {example['resumen']}")
                report.append(f"     Causa: {example['causa_raiz']}")
                if 'prioridad' in example:
                    report.append(f"     Prioridad: {example['prioridad']}")
                report.append("")
            
            report.append("=" * 60)
            report.append("")
        
        return '\n'.join(report)
    
    def _generate_technical_summary(self):
        """Genera resumen t√©cnico del an√°lisis"""
        summary = []
        summary.append("=" * 60)
        summary.append("üîß RESUMEN T√âCNICO - AN√ÅLISIS SEM√ÅNTICO")
        summary.append("=" * 60)
        summary.append(f"Fecha: {datetime.now().strftime('%Y-%m-%d')}")
        summary.append("")
        
        if not self.categories:
            summary.append("‚ùå No se generaron categor√≠as en el an√°lisis")
            return '\n'.join(summary)
        
        # Estad√≠sticas t√©cnicas
        sizes = [cat['num_incidencias'] for cat in self.categories.values()]
        summary.append("üìä ESTAD√çSTICAS T√âCNICAS:")
        summary.append(f"‚Ä¢ Total de categor√≠as: {len(self.categories)}")
        summary.append(f"‚Ä¢ Promedio incidencias por categor√≠a: {np.mean(sizes):.1f}")
        summary.append(f"‚Ä¢ Categor√≠a m√°s grande: {max(sizes)} incidencias")
        summary.append(f"‚Ä¢ Categor√≠a m√°s peque√±a: {min(sizes)} incidencias")
        summary.append("")
        
        # Top 5 por volumen
        sorted_cats = sorted(self.categories.items(), key=lambda x: x[1]['num_incidencias'], reverse=True)[:5]
        summary.append("üéØ TOP 5 CATEGOR√çAS POR VOLUMEN:")
        for i, (key, cat) in enumerate(sorted_cats, 1):
            summary.append(f"{i}. {cat['nombre']} - {cat['num_incidencias']} incidencias")
        
        summary.append("")
        summary.append("‚ö° DISTRIBUCI√ìN POR CRITICIDAD:")
        crit_dist = {}
        for cat in self.categories.values():
            crit = cat.get('criticidad', 'No evaluada')
            crit_dist[crit] = crit_dist.get(crit, 0) + 1
        
        for crit, count in crit_dist.items():
            summary.append(f"‚Ä¢ {crit}: {count} categor√≠as")
        
        return '\n'.join(summary)
    
    def export_to_json(self, filename='categorias_semanticas.json'):
        """Exporta las categor√≠as a JSON (m√©todo de compatibilidad)"""
        if not self.categories:
            print("‚ùå No hay categor√≠as para exportar")
            return
        
        # Usar el output manager para guardar
        categories_data = {
            'metadata': {
                'fecha_analisis': datetime.now().isoformat(),
                'total_categorias': len(self.categories),
                'max_categorias_permitidas': self.max_categories,
                'metodo': 'An√°lisis Sem√°ntico Refactorizado'
            },
            'categorias': self.categories
        }
        
        self.output_manager.save_categories(categories_data, filename)

def main():
    """Funci√≥n principal para ejecutar el an√°lisis sem√°ntico refactorizado"""
    print("üöÄ Iniciando Analizador Sem√°ntico Refactorizado")
    
    # Cargar datos
    try:
        df = pd.read_excel('infomation.xlsx')
        print(f"‚úÖ Datos cargados: {df.shape[0]} registros")
    except Exception as e:
        try:
            df = pd.read_csv('infomation.csv', encoding='ISO-8859-1', sep=';')
            print(f"‚úÖ Datos cargados desde CSV: {df.shape[0]} registros")
        except Exception as e2:
            print(f"‚ùå Error cargando datos: {e}")
            print(f"‚ùå Error alternativo CSV: {e2}")
            return
    
    # Filtrar registros v√°lidos
    df_clean = df[['Ticket ID', 'Resumen', 'Notas', 'Causa Ra√≠z']].copy()
    df_clean = df_clean.dropna(subset=['Causa Ra√≠z'])
    print(f"‚úÖ Registros v√°lidos: {df_clean.shape[0]}")
    
    # Crear analizador con salida organizada
    analyzer = SemanticIncidentAnalyzer(max_categories=20, output_dir="outputs_semantic")
    
    # Ejecutar an√°lisis
    categories = analyzer.analyze_incidents(df_clean)
    
    if categories:
        print("\n" + "=" * 65)
        print("‚úÖ AN√ÅLISIS SEM√ÅNTICO COMPLETADO")
        print("üìä Mejoras implementadas:")
        print("  ‚úì Estructura de carpetas organizada")
        print("  ‚úì Nomenclatura sem√°nticamente coherente")
        print("  ‚úì An√°lisis de criticidad incluido")
        print("  ‚úì Reportes categorizados por tipo")
        print("  ‚úì Caracter√≠sticas t√©cnicas evaluadas")
    else:
        print("‚ùå No se pudieron generar categor√≠as")

if __name__ == "__main__":
    main()
