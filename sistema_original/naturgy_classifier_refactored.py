#!/usr/bin/env python3
# Naturgy AI Incident Classifier - Refactored Version
"""
Sistema completo de clasificaci√≥n autom√°tica de incidencias para Naturgy Delta
Versi√≥n refactorizada con estructura de carpetas organizada y nomenclatura mejorada
"""

import pandas as pd
import numpy as np
import re
import json
import pickle
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ML Libraries
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity

# NLP Libraries
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.stem import SnowballStemmer
    NLTK_AVAILABLE = True
except ImportError:
    print("NLTK not installed. Some features may be limited.")
    NLTK_AVAILABLE = False

class OutputManager:
    """Gestiona la estructura de carpetas y archivos de salida"""
    
    def __init__(self, base_output_dir: str = "outputs"):
        self.base_dir = Path(base_output_dir)
        self.setup_directory_structure()
    
    def setup_directory_structure(self):
        """Crea la estructura de carpetas necesaria"""
        directories = {
            'models': self.base_dir / 'models',
            'reports': self.base_dir / 'reports', 
            'data': self.base_dir / 'data',
            'logs': self.base_dir / 'logs'
        }
        
        for dir_name, dir_path in directories.items():
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"üìÅ Directorio {dir_name} preparado: {dir_path}")
    
    def get_path(self, category: str, filename: str) -> Path:
        """Obtiene la ruta completa para un archivo seg√∫n su categor√≠a"""
        return self.base_dir / category / filename
    
    def save_model(self, model_data: dict, filename: str = None):
        """Guarda modelo en la carpeta models"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"naturgy_model_{timestamp}.pkl"
        
        filepath = self.get_path('models', filename)
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"üíæ Modelo guardado: {filepath}")
        return filepath
    
    def save_json_data(self, data: dict, filename: str):
        """Guarda datos JSON en la carpeta data"""
        filepath = self.get_path('data', filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üìä Datos JSON guardados: {filepath}")
        return filepath
    
    def save_report(self, content: str, filename: str):
        """Guarda reporte de texto en la carpeta reports"""
        filepath = self.get_path('reports', filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"üìÑ Reporte guardado: {filepath}")
        return filepath

class CategoryNamingEngine:
    """Motor de nomenclatura inteligente para categor√≠as"""
    
    def __init__(self):
        self.setup_naming_rules()
    
    def setup_naming_rules(self):
        """Configura reglas de nomenclatura sem√°ntica"""
        
        # Patrones t√©cnicos principales
        self.technical_patterns = {
            'infraestructura': {
                'keywords': ['servidor', 'infraestructura', 'sistema', 'red', 'conexion', 'servicio'],
                'base_name': 'Infraestructura'
            },
            'datos': {
                'keywords': ['datos', 'data', 'base', 'carga', 'actualizaci√≥n', 'masiva', 'informaci√≥n'],
                'base_name': 'Gesti√≥n_Datos'
            },
            'comunicacion': {
                'keywords': ['comunicacion', 'notificacion', 'envio', 'correo', 'mensaje', 'alerta'],
                'base_name': 'Comunicaciones'
            },
            'procesos': {
                'keywords': ['batch', 'proceso', 'job', 'ejecucion', 'automatico', 'programado'],
                'base_name': 'Procesos_Batch'
            },
            'consultas': {
                'keywords': ['consulta', 'busqueda', 'listado', 'extraccion', 'reporte', 'informaci√≥n'],
                'base_name': 'Consultas_Funcionales'
            },
            'errores': {
                'keywords': ['error', 'fallo', 'excepcion', 'timeout', 'crash', 'bug'],
                'base_name': 'Errores_Sistema'
            },
            'facturacion': {
                'keywords': ['factura', 'facturacion', 'importe', 'cobro', 'pago', 'recibo'],
                'base_name': 'Facturaci√≥n'
            },
            'contratos': {
                'keywords': ['contrato', 'alta', 'baja', 'modificacion', 'cambio', 'gestion'],
                'base_name': 'Gesti√≥n_Contratos'
            }
        }
        
        # Sufijos para especificar subtipos
        self.subtypes = {
            'critico': ['cr√≠tico', 'urgente', 'bloqueante', 'alto'],
            'masivo': ['masivo', 'm√∫ltiple', 'lote', 'bulk'],
            'automatico': ['autom√°tico', 'batch', 'programado', 'scheduled'],
            'manual': ['manual', 'individual', 'espec√≠fico', 'puntual']
        }
    
    def generate_semantic_name(self, cluster_df: pd.DataFrame, cluster_id: int, 
                             keywords: List[str]) -> str:
        """Genera nombre sem√°nticamente coherente"""
        
        # Combinar todo el texto para an√°lisis
        all_text = ' '.join([
            ' '.join(cluster_df['Resumen'].fillna('').astype(str)),
            ' '.join(cluster_df.get('Notas', pd.Series([])).fillna('').astype(str)),
            ' '.join(cluster_df.get('Tipo de ticket', pd.Series([])).fillna('').astype(str))
        ]).lower()
        
        # Identificar patr√≥n t√©cnico principal
        main_pattern = self._identify_main_pattern(all_text)
        
        # Identificar subtipo si existe
        subtype = self._identify_subtype(all_text)
        
        # Generar nombre base
        if main_pattern:
            base_name = self.technical_patterns[main_pattern]['base_name']
        else:
            # Fallback: usar keywords m√°s distintivos
            base_name = self._generate_fallback_name(keywords, cluster_df)
        
        # A√±adir subtipo si se identific√≥
        if subtype:
            semantic_name = f"{base_name}_{subtype.title()}"
        else:
            semantic_name = base_name
        
        # A√±adir especificador si es necesario para evitar duplicados
        size_modifier = self._get_size_modifier(len(cluster_df))
        if size_modifier:
            semantic_name = f"{semantic_name}_{size_modifier}"
        
        return semantic_name.replace('_', ' ')
    
    def _identify_main_pattern(self, text: str) -> Optional[str]:
        """Identifica el patr√≥n t√©cnico principal"""
        pattern_scores = {}
        
        for pattern_name, pattern_info in self.technical_patterns.items():
            score = 0
            for keyword in pattern_info['keywords']:
                score += text.count(keyword.lower())
            pattern_scores[pattern_name] = score
        
        # Retornar el patr√≥n con mayor puntuaci√≥n si supera el umbral
        max_pattern = max(pattern_scores.items(), key=lambda x: x[1])
        return max_pattern[0] if max_pattern[1] > 0 else None
    
    def _identify_subtype(self, text: str) -> Optional[str]:
        """Identifica subtipo espec√≠fico"""
        for subtype_name, subtype_keywords in self.subtypes.items():
            for keyword in subtype_keywords:
                if keyword.lower() in text:
                    return subtype_name
        return None
    
    def _generate_fallback_name(self, keywords: List[str], cluster_df: pd.DataFrame) -> str:
        """Genera nombre alternativo basado en keywords distintivos"""
        if not keywords:
            return f"Categoria_Tecnica"
        
        # Usar las 2 palabras m√°s distintivas
        main_keywords = keywords[:2]
        
        # Limpiar y formatear
        cleaned_keywords = []
        for keyword in main_keywords:
            if len(keyword) > 2:
                cleaned_keywords.append(keyword.title())
        
        if len(cleaned_keywords) >= 2:
            return f"{cleaned_keywords[0]}_{cleaned_keywords[1]}"
        elif len(cleaned_keywords) == 1:
            return f"Incidencias_{cleaned_keywords[0]}"
        else:
            return "Categoria_Generica"
    
    def _get_size_modifier(self, size: int) -> Optional[str]:
        """Obtiene modificador basado en el tama√±o del cluster"""
        if size > 100:
            return "Frecuentes"
        elif size < 10:
            return "Espec√≠ficas"
        return None

class NaturgyIncidentClassifier:
    """Pipeline completo para clasificaci√≥n autom√°tica de incidencias - Versi√≥n Refactorizada"""
    
    def __init__(self, config: Optional[Dict] = None, output_dir: str = "outputs"):
        self.config = config or self._default_config()
        self.output_manager = OutputManager(output_dir)
        self.naming_engine = CategoryNamingEngine()
        
        self.preprocessor = TextPreprocessor(self.config)
        self.clusterer = IncidentClusterer(self.config)
        self.classifier = PredictiveClassifier(self.config)
        self.entity_extractor = EntityExtractor()
        
        self.is_trained = False
        self.incident_types = {}
        self.model_metrics = {}
        
    def _default_config(self) -> Dict:
        """Configuraci√≥n por defecto del sistema"""
        return {
            'max_clusters': 50,
            'min_cluster_size': 20,
            'tfidf_max_features': 8000,
            'tfidf_min_df': 3,
            'tfidf_max_df': 0.7,
            'random_state': 42,
            'use_llm': False,
            'model_type': 'random_forest',
            'cv_folds': 5,
            'use_hierarchical': True,
            'silhouette_threshold': 0.1
        }
    
    def train_pipeline(self, data_path: str) -> Dict[str, Any]:
        """Entrena el pipeline completo"""
        print("üöÄ Iniciando entrenamiento del pipeline de clasificaci√≥n...")
        
        # 1. Cargar y limpiar datos
        print("üìä Cargando datos...")
        df = self._load_data(data_path)
        
        # 2. Preprocesamiento
        print("üßπ Preprocesando texto...")
        df_processed = self.preprocessor.process_dataframe(df)
        
        # 3. Extracci√≥n de entidades
        print("üîç Extrayendo entidades...")
        df_processed = self.entity_extractor.extract_entities(df_processed)
        
        # 4. Clustering inicial
        print("üéØ Realizando clustering inicial...")
        cluster_results = self.clusterer.cluster_incidents(df_processed)
        
        # 5. Entrenamiento del modelo predictivo
        print("ü§ñ Entrenando modelo predictivo...")
        model_results = self.classifier.train_model(
            df_processed, cluster_results['labels']
        )
        
        # 6. Generar tipos de incidencia con nomenclatura mejorada
        print("üìã Generando definiciones de tipos...")
        self.incident_types = self._generate_incident_types_improved(
            df_processed, cluster_results
        )
        
        # 7. Evaluaci√≥n del sistema
        print("üìà Evaluando performance...")
        self.model_metrics = self._evaluate_system(
            df_processed, cluster_results, model_results
        )
        
        self.is_trained = True
        
        # 8. Guardar modelo y resultados con estructura organizada
        self._save_results_organized(cluster_results, model_results)
        
        return {
            'incident_types': self.incident_types,
            'metrics': self.model_metrics,
            'cluster_results': cluster_results,
            'model_results': model_results
        }
    
    def _generate_incident_types_improved(self, df: pd.DataFrame, 
                                        cluster_results: Dict) -> Dict[str, Dict]:
        """Genera definiciones de tipos con nomenclatura mejorada"""
        types = {}
        labels = cluster_results['labels']
        
        for cluster_id in np.unique(labels):
            if cluster_id == -1:  # Ruido en clustering
                continue
                
            cluster_mask = labels == cluster_id
            cluster_df = df[cluster_mask]
            
            # Extraer palabras clave distintivas
            keywords = self._extract_distinctive_keywords(cluster_df)
            
            # Generar nombre sem√°nticamente coherente
            semantic_name = self.naming_engine.generate_semantic_name(
                cluster_df, cluster_id, keywords
            )
            
            # Generar descripci√≥n mejorada
            description = self._generate_enhanced_description(cluster_df, semantic_name)
            
            type_info = {
                'nombre': semantic_name,
                'descripcion': description,
                'num_incidencias': len(cluster_df),
                'palabras_clave': keywords[:10],  # Top 10 keywords
                'tipos_principales': self._get_top_ticket_types(cluster_df),
                'ejemplos': self._get_representative_examples(cluster_df),
                'patrones_identificados': self._identify_patterns(cluster_df),
                'nivel_criticidad': self._assess_criticality(cluster_df)
            }
            
            # Usar nombre sem√°ntico como clave
            safe_key = re.sub(r'[^\w\s-]', '', semantic_name).replace(' ', '_').lower()
            types[safe_key] = type_info
            
        return types
    
    def _generate_enhanced_description(self, cluster_df: pd.DataFrame, 
                                     semantic_name: str) -> str:
        """Genera descripci√≥n mejorada basada en el an√°lisis sem√°ntico"""
        size = len(cluster_df)
        
        # Analizar palabras clave m√°s frecuentes para contexto
        top_keywords = self._extract_distinctive_keywords(cluster_df)[:3]
        
        description = f"Categor√≠a '{semantic_name}' que agrupa {size} incidencias "
        
        # A√±adir contexto espec√≠fico basado en el nombre
        if 'Infraestructura' in semantic_name:
            description += "relacionadas con problemas de infraestructura, servicios y conectividad del sistema."
        elif 'Datos' in semantic_name:
            description += "relacionadas con gesti√≥n, actualizaci√≥n y procesamiento de datos."
        elif 'Comunicaciones' in semantic_name:
            description += "relacionadas con notificaciones, env√≠os y sistemas de comunicaci√≥n."
        elif 'Procesos' in semantic_name:
            description += "relacionadas con procesos automatizados, jobs y tareas programadas."
        elif 'Consultas' in semantic_name:
            description += "relacionadas con b√∫squedas, listados y extracci√≥n de informaci√≥n."
        elif 'Errores' in semantic_name:
            description += "relacionadas con fallos t√©cnicos, excepciones y errores del sistema."
        elif 'Facturaci√≥n' in semantic_name:
            description += "relacionadas con procesos de facturaci√≥n, cobros y aspectos econ√≥micos."
        elif 'Contratos' in semantic_name:
            description += "relacionadas con gesti√≥n de contratos, altas, bajas y modificaciones."
        else:
            description += "con caracter√≠sticas t√©cnicas y funcionales espec√≠ficas."
        
        # A√±adir informaci√≥n sobre t√©rminos t√©cnicos m√°s frecuentes
        if top_keywords:
            description += f" T√©rminos t√©cnicos frecuentes: '{', '.join(top_keywords[:3])}'."
        
        return description
    
    def _identify_patterns(self, cluster_df: pd.DataFrame) -> List[str]:
        """Identifica patrones espec√≠ficos en el cluster"""
        patterns = []
        
        # Analizar texto combinado
        all_text = ' '.join(cluster_df['Resumen'].fillna('').astype(str)).lower()
        
        # Patrones temporales
        if any(word in all_text for word in ['nocturno', 'madrugada', 'fin semana']):
            patterns.append('Incidencias fuera de horario laboral')
        
        # Patrones de volumen
        if any(word in all_text for word in ['masivo', 'm√∫ltiple', 'varios']):
            patterns.append('Afectaci√≥n m√∫ltiple o masiva')
        
        # Patrones de urgencia
        if any(word in all_text for word in ['urgente', 'cr√≠tico', 'bloqueo']):
            patterns.append('Requiere atenci√≥n prioritaria')
        
        # Patrones de automatizaci√≥n
        if any(word in all_text for word in ['autom√°tico', 'batch', 'programado']):
            patterns.append('Procesos automatizados')
        
        return patterns
    
    def _assess_criticality(self, cluster_df: pd.DataFrame) -> str:
        """Eval√∫a el nivel de criticidad del cluster"""
        all_text = ' '.join(cluster_df['Resumen'].fillna('').astype(str)).lower()
        
        # Indicadores de alta criticidad
        high_criticality_indicators = ['cr√≠tico', 'urgente', 'bloqueo', 'ca√≠da', 'fallo total']
        # Indicadores de media criticidad  
        medium_criticality_indicators = ['error', 'problema', 'incidencia', 'fallo']
        # Indicadores de baja criticidad
        low_criticality_indicators = ['consulta', 'informaci√≥n', 'duda', 'aclaraci√≥n']
        
        high_score = sum(1 for indicator in high_criticality_indicators if indicator in all_text)
        medium_score = sum(1 for indicator in medium_criticality_indicators if indicator in all_text)
        low_score = sum(1 for indicator in low_criticality_indicators if indicator in all_text)
        
        if high_score > 0:
            return 'Alta'
        elif medium_score > low_score:
            return 'Media'
        else:
            return 'Baja'
    
    def _save_results_organized(self, cluster_results: Dict, model_results: Dict):
        """Guarda los resultados con estructura organizada"""
        
        # 1. Guardar modelo entrenado
        model_data = {
            'preprocessor': self.preprocessor,
            'clusterer': self.clusterer,
            'classifier': self.classifier,
            'entity_extractor': self.entity_extractor,
            'incident_types': self.incident_types,
            'config': self.config,
            'is_trained': self.is_trained,
            'timestamp': datetime.now().isoformat()
        }
        
        self.output_manager.save_model(model_data)
        
        # 2. Guardar an√°lisis completo en JSON
        analysis_data = {
            'metadata': {
                'fecha_analisis': datetime.now().isoformat(),
                'total_categorias': len(self.incident_types),
                'metodo_clustering': 'KMeans',
                'metodo_clasificacion': self.config['model_type']
            },
            'tipos_de_incidencia': self.incident_types,
            'metricas_modelo': self.model_metrics,
            'cluster_info': {
                'num_clusters': cluster_results['n_clusters'],
                'silhouette_score': cluster_results['silhouette_score'],
                'cluster_sizes': cluster_results['cluster_sizes']
            }
        }
        
        self.output_manager.save_json_data(
            analysis_data, 
            'analisis_completo_naturgy.json'
        )
        
        # 3. Guardar reporte legible
        report_content = self._generate_comprehensive_report()
        self.output_manager.save_report(
            report_content,
            'reporte_analisis_naturgy.txt'
        )
        
        # 4. Guardar resumen ejecutivo
        executive_summary = self._generate_executive_summary()
        self.output_manager.save_report(
            executive_summary,
            'resumen_ejecutivo.txt'
        )
    
    def _generate_comprehensive_report(self) -> str:
        """Genera reporte completo y detallado"""
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("üìä AN√ÅLISIS INTEGRAL DE INCIDENCIAS NATURGY DELTA")
        report_lines.append("=" * 80)
        report_lines.append(f"üìÖ Fecha de an√°lisis: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"üéØ Total de categor√≠as identificadas: {len(self.incident_types)}")
        report_lines.append(f"ü§ñ Precisi√≥n del modelo: {self.model_metrics.get('model_accuracy', 0.0):.3f}")
        report_lines.append(f"üìä Silhouette Score: {self.model_metrics.get('silhouette_score', 0.0):.3f}")
        report_lines.append("")
        
        # Ordenar por n√∫mero de incidencias
        sorted_types = sorted(
            self.incident_types.items(),
            key=lambda x: x[1]['num_incidencias'],
            reverse=True
        )
        
        report_lines.append("üè∑Ô∏è  CATEGOR√çAS IDENTIFICADAS (ordenadas por volumen)")
        report_lines.append("=" * 80)
        
        for type_key, type_info in sorted_types:
            report_lines.append(f"\nüìã {type_info['nombre'].upper()}")
            report_lines.append("-" * 60)
            report_lines.append(f"üìä Incidencias: {type_info['num_incidencias']}")
            report_lines.append(f"‚ö° Criticidad: {type_info.get('nivel_criticidad', 'No evaluada')}")
            report_lines.append(f"üìù Descripci√≥n: {type_info['descripcion']}")
            report_lines.append(f"üîë Palabras clave: {', '.join(type_info['palabras_clave'][:5])}")
            
            if type_info.get('patrones_identificados'):
                report_lines.append(f"üîç Patrones: {', '.join(type_info['patrones_identificados'])}")
            
            if type_info['tipos_principales']:
                report_lines.append(f"üìÇ Tipos principales: {', '.join(type_info['tipos_principales'][:3])}")
            
            report_lines.append("\nüìã Ejemplos representativos:")
            for i, example in enumerate(type_info['ejemplos'][:3], 1):
                report_lines.append(f"  {i}. [{example['id']}] {example['resumen']}")
            
            report_lines.append("\n" + "=" * 60)
        
        return '\n'.join(report_lines)
    
    def _generate_executive_summary(self) -> str:
        """Genera resumen ejecutivo para directivos"""
        summary_lines = []
        summary_lines.append("=" * 60)
        summary_lines.append("üìä RESUMEN EJECUTIVO - AN√ÅLISIS DE INCIDENCIAS")
        summary_lines.append("=" * 60)
        summary_lines.append(f"Fecha: {datetime.now().strftime('%Y-%m-%d')}")
        summary_lines.append("")
        
        # Estad√≠sticas clave
        total_categories = len(self.incident_types)
        high_criticality = sum(1 for t in self.incident_types.values() if t.get('nivel_criticidad') == 'Alta')
        
        summary_lines.append("üéØ RESULTADOS CLAVE:")
        summary_lines.append(f"‚Ä¢ Total de categor√≠as identificadas: {total_categories}")
        summary_lines.append(f"‚Ä¢ Categor√≠as de alta criticidad: {high_criticality}")
        summary_lines.append(f"‚Ä¢ Precisi√≥n del modelo: {self.model_metrics.get('model_accuracy', 0.0):.1%}")
        summary_lines.append("")
        
        # Top 5 categor√≠as por volumen
        sorted_types = sorted(
            self.incident_types.items(),
            key=lambda x: x[1]['num_incidencias'],
            reverse=True
        )[:5]
        
        summary_lines.append("üìä TOP 5 CATEGOR√çAS POR VOLUMEN:")
        for i, (key, info) in enumerate(sorted_types, 1):
            summary_lines.append(f"{i}. {info['nombre']} - {info['num_incidencias']} incidencias")
        
        summary_lines.append("")
        summary_lines.append("üí° RECOMENDACIONES:")
        summary_lines.append("‚Ä¢ Priorizar atenci√≥n a categor√≠as de alta criticidad")
        summary_lines.append("‚Ä¢ Implementar automatizaci√≥n en categor√≠as m√°s frecuentes")
        summary_lines.append("‚Ä¢ Desarrollar procedimientos espec√≠ficos por categor√≠a")
        
        return '\n'.join(summary_lines)
    
    # Resto de m√©todos heredados de la clase original...
    def classify_incident(self, incident_text: str, 
                         additional_fields: Optional[Dict] = None) -> Dict[str, Any]:
        """Clasifica una nueva incidencia"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")
        
        # Sistema de preclasificaci√≥n con reglas sem√°nticamente coherentes
        combined_text = incident_text.lower()
        if additional_fields:
            for key, value in additional_fields.items():
                if value and str(value).strip().lower() != 'nan':
                    combined_text += f" {str(value).lower()}"
        
        # Aplicar reglas de preclasificaci√≥n espec√≠ficas
        classification_result = self._apply_semantic_rules(combined_text)
        if classification_result:
            return classification_result
            
        # Crear DataFrame temporal para procesamiento
        temp_df = pd.DataFrame({
            'Resumen': [incident_text],
            'Notas': [additional_fields.get('notas', '') if additional_fields else ''],
            'Tipo de ticket': [additional_fields.get('tipo_ticket', '') if additional_fields else '']
        })
        
        # Procesar texto
        processed_df = self.preprocessor.process_dataframe(temp_df)
        processed_df = self.entity_extractor.extract_entities(processed_df)
        
        # Clasificar
        prediction = self.classifier.predict(processed_df)
        
        # Control de confianza para modelo predictivo - Si confianza < 0.70, clasificar como "sin determinar"
        confidence = prediction[1] if len(prediction) > 1 else 0.0
        if confidence < 0.70:
            return {
                'predicted_type': 'sin_determinar',
                'confidence': confidence,
                'type_info': {
                    'nombre': 'Sin determinar',
                    'descripcion': f'Confianza del modelo predictivo ({confidence:.2f}) insuficiente para clasificaci√≥n autom√°tica. Requiere revisi√≥n manual.',
                    'palabras_clave': ['baja confianza', 'modelo predictivo'],
                    'nivel_criticidad': 'No evaluada'
                },
                'extracted_entities': processed_df['entities'].iloc[0] if 'entities' in processed_df.columns else {},
                'processed_text': processed_df['combined_text'].iloc[0] if 'combined_text' in processed_df.columns else ''
            }
        
        # Obtener informaci√≥n del tipo
        incident_type = self.incident_types.get(prediction[0], {})
        
        return {
            'predicted_type': prediction[0],
            'confidence': confidence,
            'type_info': incident_type,
            'extracted_entities': processed_df['entities'].iloc[0] if 'entities' in processed_df.columns else {},
            'processed_text': processed_df['combined_text'].iloc[0] if 'combined_text' in processed_df.columns else ''
        }
    
    def _load_data(self, data_path: str) -> pd.DataFrame:
        """Carga datos desde Excel"""
        try:
            df = pd.read_excel(data_path)
            print(f"‚úÖ Datos cargados: {df.shape[0]} registros, {df.shape[1]} columnas")
            return df
        except Exception as e:
            print(f"‚ùå Error cargando datos: {e}")
            raise
    
    def _extract_distinctive_keywords(self, cluster_df: pd.DataFrame) -> List[str]:
        """Extrae palabras clave M√ÅS distintivas (no las m√°s comunes)"""
        # Combinar todo el texto del cluster
        cluster_text = ' '.join(cluster_df['Resumen'].fillna('').astype(str))
        
        # Tokenizar
        words = re.findall(r'\b[a-zA-Z0-9]{3,}\b', cluster_text.lower())
        
        # Stop words expandidas
        expanded_stops = {
            'de', 'la', 'el', 'en', 'y', 'a', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 
            'por', 'con', 'su', 'para', 'como', 'las', 'del', 'los', 'un', 'una', 'al', 'del',
            'que', 'fue', 'son', 'han', 'muy', 'm√°s', 'son', 'este', 'esta', 'ese', 'esa',
            'tiene', 'hacer', 'todo', 'a√±o', 'd√≠a', 'mes', 'vez', 'caso', 'forma', 'parte'
        }
        
        # Filtrar y contar
        filtered_words = [w for w in words if w not in expanded_stops and len(w) > 2]
        word_freq = pd.Series(filtered_words).value_counts()
        
        # Retornar las 15 m√°s distintivas
        return word_freq.head(15).index.tolist()
    
    def _get_top_ticket_types(self, cluster_df: pd.DataFrame) -> List[str]:
        """Obtiene los principales tipos de ticket"""
        if 'Tipo de ticket' in cluster_df.columns:
            return cluster_df['Tipo de ticket'].value_counts().head(3).index.tolist()
        return []
    
    def _get_representative_examples(self, cluster_df: pd.DataFrame) -> List[Dict]:
        """Obtiene ejemplos representativos"""
        examples = []
        for _, row in cluster_df.head(3).iterrows():
            example = {
                'id': row.get('Ticket ID', f'idx_{row.name}'),
                'resumen': str(row.get('Resumen', ''))[:150] + '...' if len(str(row.get('Resumen', ''))) > 150 else str(row.get('Resumen', '')),
                'tipo_ticket': str(row.get('Tipo de ticket', ''))
            }
            examples.append(example)
        return examples
    
    def _apply_semantic_rules(self, combined_text: str) -> Optional[Dict[str, Any]]:
        """Aplica reglas sem√°nticas de preclasificaci√≥n espec√≠ficas con diccionario extendido"""
        
        # üîç PREPROCESAMIENTO: Detectar c√≥digos CUPS (ES + n√∫meros) y reemplazar por 'cups'
        import re
        cups_pattern = r'\bES\d{4}\d{4}\d{4}\d{4}[A-Z0-9]*\b'
        if re.search(cups_pattern, combined_text):
            combined_text += ' cups'  # A√±adir keyword cups cuando se detecte el patr√≥n
        
        # üéØ SISTEMA DEFINITIVO DE CLASIFICACI√ìN NATURGY - CATEGOR√çAS T√âCNICAS EXPANDIDAS
        keywords = {
            # 1Ô∏è‚É£ Gesti√≥n de CUPS (EXPANDIDO)
            "gestion_cups": [
                "cups", "alta cups", "baja cups", "activar cups", "procesar baja", "procesar activaci√≥n",
                "modificar direcci√≥n cups", "asociar sector suministro", "desvincular cups",
                "error vincular cups", "regularizar cups", "rechazos cups", "modificar cups",
                "c√≥digo distribuidora", "ubicaci√≥n cups", "cambio de datos cups"
            ],

            # 2Ô∏è‚É£ Montaje/Desmontaje/Equipos de medida
            "montaje_desmontaje_equipos": [
                "montaje", "desmontaje", "levantar aparato", "alta de aparato",
                "baja de aparato", "cambio de aparato", "error levantamiento",
                "no se puede eliminar aparato", "equipo de lectura", "aparato bloqueado",
                "eliminar aparato", "levantamiento de aparato", "retirada de equipo"
            ],

            # 3Ô∏è‚É£ Errores de c√°lculo/facturaci√≥n (EXPANDIDO)
            "errores_calculo_facturacion": [
                "error al calcular", "no permite calcular", "suministro no listo para facturar",
                "tipdet", "passthrough", "p√©rdidas", "curva y cierre", "orden de c√°lculo",
                "oc", "ol", "java.lang.nullpointerexception", "error edm",
                "error gen√©rico de c√°lculo", "error gen√©rico del c√°lculo", "bloqueo facturaci√≥n", 
                "fallo c√°lculo", "no calculable", "factura", "emisi√≥n de factura"
            ],

            # 4Ô∏è‚É£ Estados de c√°lculo/facturaci√≥n
            "estados_calculo_facturacion": [
                "no calculable", "calculable", "bloqueado", "pendiente de facturar",
                "baja pendiente", "estado incoherente", "apartada", "desbloquear oc",
                "desbloquear ol", "bloquear", "desbloquear", "no tratable"
            ],

            # 5Ô∏è‚É£ Lecturas y mediciones
            "lecturas_mediciones": [
                "lectura de baja", "lectura de alta", "modificar lectura", "anular lectura",
                "solape", "lecturas no coinciden", "error lectura", "nm3", "pcs",
                "lecturas", "medici√≥n", "alta manual de lecturas"
            ],

            # 6Ô∏è‚É£ Direcciones y datos de cliente (EXPANDIDO)
            "direcciones_datos_cliente": [
                "modificar direcci√≥n", "direcci√≥n incorrecta", "correcci√≥n direcci√≥n",
                "datos titular", "cambiar nombre", "cambiar nif", "correo electr√≥nico",
                "tel√©fono", "actualizaci√≥n direcci√≥n", "datos del titular", "nif", "dni",
                "nombre cliente", "email", "datos incompletos", "cambio de direcci√≥n",
                "actualizaci√≥n datos", "ofuscaci√≥n email"
            ],

            # 7Ô∏è‚É£ Cambio de titularidad
            "cambio_titularidad": [
                "cambio de titular", "cambio titular sin subrogaci√≥n", "error cambio titular",
                "validaci√≥n cambio titular", "cambio titular", "subrogaci√≥n", "titular"
            ],

            # 8Ô∏è‚É£ Ofertas y contrataci√≥n
            "ofertas_contratacion": [
                "oferta en elaboraci√≥n", "aceptar oferta", "firmar oferta", "validar oferta",
                "error cau", "adenda", "alta oferta", "error tipo oferta", "contrataci√≥n",
                "no se puede validar oferta", "imposibilidad validar oferta", "modificar oferta"
            ],

            # 9Ô∏è‚É£ Tarifas y productos (EXPANDIDO)
            "tarifas_productos": [
                "tarifa comercial", "tarifa incorrecta", "cambiar producto", "producto activo",
                "cuida b√°sico", "cuida luz", "mantenimiento", "servicio contratado", "producto",
                "cambiar IEH", "modificar tarifa", "modificar producto", "cambiar precio",
                "actualizar oferta", "c√≥digo de oferta", "ID oferta", "producto asociado",
                "par√°metro oferta", "condiciones oferta"
            ],

            # üîü Gesti√≥n de contratos (NUEVA CATEGOR√çA)
            "gestion_contratos": [
                "modificar oferta", "actualizar contrato", "cambiar datos contrato",
                "editar oferta", "n√∫mero oferta", "actualizaci√≥n condiciones", "contrato",
                "gesti√≥n contratos", "datos contrato", "condiciones contrato"
            ],

            # üîü Bono social y vulnerabilidad
            "bono_social_vulnerabilidad": [
                "bono social", "vulnerable", "vulnerabilidad", "tur vulnerable severo",
                "renovaci√≥n bono social", "bono"
            ],

            # 1Ô∏è‚É£1Ô∏è‚É£ Rechazos y bloqueos
            "rechazos_bloqueos": [
                "rechazo pendiente", "cancelar rechazo", "error rechazo", "bloqueo suministro",
                "bloqueo cobros", "rechazo en vuelo", "rechazo", "bloqueo", "gestionar rechazo",
                "no permite gestionar rechazo"
            ],

            # 1Ô∏è‚É£2Ô∏è‚É£ Cobros y pagos
            "cobros_pagos": [
                "cobros", "ventanilla", "devoluci√≥n", "responsabilidad de cobro",
                "pase a fallido", "impago", "duplicidad pagos", "error contabilidad",
                "lote de cobro", "pago", "facturas vencidas"
            ],

            # 1Ô∏è‚É£3Ô∏è‚É£ Batch/Procesos autom√°ticos (EXPANDIDO)
            "batch_procesos_automaticos": [
                "fail", "long running", "ended not ok", "ftp env√≠o", "batch",
                "timeout", "script", "sysout", "proceso detenido", "excessive",
                "job", "interfaz", "ftp", "fallo batch", "procesos autom√°ticos", "error proceso"
            ],

            # 1Ô∏è‚É£4Ô∏è‚É£ Extracciones e informes
            "extracciones_informes": [
                "extracci√≥n", "informe", "listado", "recuento", "consulta",
                "descarga datos", "query consumos", "extracci√≥n aparatos", "extracci√≥n telemedida",
                "solicitud informe", "obtenci√≥n de datos", "extracci√≥n facturas"
            ],

            # 1Ô∏è‚É£5Ô∏è‚É£ Telemedida y medici√≥n remota
            "telemedida_medicion_remota": [
                "telemedida", "dispone telemedida", "medici√≥n remota", "flag telemedida",
                "indicador telemedida"
            ],

            # 1Ô∏è‚É£6Ô∏è‚É£ Errores XML/mensajer√≠a
            "errores_xml_mensajeria": [
                "xml incorrecto", "error mensajer√≠a", "error formato xml", "no genera xml",
                "mensaje en incidencia", "xml", "mensajer√≠a"
            ],

            # 1Ô∏è‚É£7Ô∏è‚É£ Integraciones externas (EXPANDIDO)
            "integraciones_externas": [
                "gnclick", "salesforce", "markets", "sincronizaci√≥n", "no viaja dato",
                "integraci√≥n externa", "interfaz externa", "replicaci√≥n", "no replicado",
                "actualizar puntos suministro"
            ],

            # 1Ô∏è‚É£8Ô∏è‚É£ Campa√±as y marketing
            "campanas_marketing": [
                "campa√±a simulada", "campa√±a asnef", "forzar campa√±a", "error campa√±a",
                "campa√±a", "eliminar campa√±as simuladas"
            ],

            # 1Ô∏è‚É£9Ô∏è‚É£ Plantillas y documentaci√≥n (EXPANDIDO)
            "plantillas_documentacion": [
                "plantillas", "subir plantilla", "eliminar plantilla", "documentaci√≥n",
                "error descarga documentos", "error carga documentos", "plantilla",
                "documento", "subir contrato", "retirar plantilla", "modificar documento", 
                "pdf factura", "generar documento", "pintado pdf", "visualizaci√≥n factura"
            ],

            # 2Ô∏è‚É£0Ô∏è‚É£ Consultas y soporte funcional (EXPANDIDO)
            "consultas_soporte_funcional": [
                "consulta tablas", "sql", "funcionamiento batch", "criterios facturaci√≥n",
                "soporte funcional", "consulta", "funcionamiento", "criterios",
                "pregunta", "validaci√≥n calendario", "solicitud ayuda", "informaci√≥n proceso", 
                "visualizar contratos", "fusionar cliente", "cliente clonado", "cliente duplicado"
            ],

            # 2Ô∏è‚É£1Ô∏è‚É£ Gesti√≥n de usuarios (EXPANDIDA)
            "gestion_usuarios": [
                "reasignaci√≥n usuario", "usuario gen√©rico", "cambio de usuario",
                "permisos", "acceso denegado", "usuario", "asignaci√≥n", "reasignar",
                "permisos usuario", "rol usuario", "acceso usuario",
                "crear usuario", "alta usuario", "nuevo usuario"
            ],

            # 2Ô∏è‚É£2Ô∏è‚É£ Gestiones internas administrativas (NUEVA CATEGOR√çA)
            "gestiones_internas_administrativas": [
                "crear puesto de trabajo", "generar anexo", "alta interna", "tarea interna",
                "gesti√≥n administrativa", "crear registro", "crear puesto", "crear tarea",
                "puesto de trabajo", "anexo", "gesti√≥n interna", "tarea administrativa"
            ],

            # 2Ô∏è‚É£3Ô∏è‚É£ Gesti√≥n de ofertas (NUEVA CATEGOR√çA)
            "gestion_ofertas": [
                "activar oferta", "activar l√≠nea oferta", "modificar oferta", "validar oferta",
                "condiciones oferta", "par√°metro oferta", "cambiar IEH", "actualizar oferta",
                "anexo oferta", "pre-oferta", "l√≠nea oferta", "activaci√≥n oferta",
                "par√°metros oferta", "generar anexo pre-oferta"
            ],

            # 2Ô∏è‚É£4Ô∏è‚É£ Sincronizaci√≥n de datos (NUEVA CATEGOR√çA)
            "sincronizacion_datos": [
                "replicaci√≥n", "no replicado", "actualizar puntos", "sincronizar datos",
                "sincronizaci√≥n", "datos no sincronizados", "no se han replicado",
                "puntos de suministro", "replicar datos", "actualizar puntos de suministro"
            ]
        }
        
        # üéØ MAPEO DEFINITIVO DE 20 CATEGOR√çAS T√âCNICAS NATURGY
        category_info = {
            # 1Ô∏è‚É£ Gesti√≥n de CUPS
            "gestion_cups": {
                'nombre': 'Gesti√≥n de CUPS',
                'descripcion': 'Todo ticket que mencione expl√≠citamente un CUPS y acciones sobre √©l (alta, baja, cambio de datos, asociaci√≥n/desvinculaci√≥n)',
                'criticidad': 'Media'
            },

            # 2Ô∏è‚É£ Montaje/Desmontaje/Equipos de medida
            "montaje_desmontaje_equipos": {
                'nombre': 'Montaje/Desmontaje/Equipos de medida',
                'descripcion': 'Incidencias sobre aparatos o contadores (montaje, desmontaje, levantamiento, error en equipos)',
                'criticidad': 'Alta'
            },

            # 3Ô∏è‚É£ Errores de c√°lculo/facturaci√≥n
            "errores_calculo_facturacion": {
                'nombre': 'Errores de c√°lculo/facturaci√≥n',
                'descripcion': 'Mensajes de error durante el c√°lculo de √≥rdenes de c√°lculo o facturas',
                'criticidad': 'Alta'
            },

            # 4Ô∏è‚É£ Estados de c√°lculo/facturaci√≥n
            "estados_calculo_facturacion": {
                'nombre': 'Estados de c√°lculo/facturaci√≥n',
                'descripcion': 'Cambios de estado (no calculable, calculable, bloqueado) o incoherencias en estado de c√°lculo',
                'criticidad': 'Media'
            },

            # 5Ô∏è‚É£ Lecturas y mediciones
            "lecturas_mediciones": {
                'nombre': 'Lecturas y mediciones',
                'descripcion': 'Problemas con lecturas (baja, alta, modificar, anular, solapes, PCS)',
                'criticidad': 'Media'
            },

            # 6Ô∏è‚É£ Direcciones y datos de cliente
            "direcciones_datos_cliente": {
                'nombre': 'Direcciones y datos de cliente',
                'descripcion': 'Cambios o correcciones en direcci√≥n, nombre, NIF, email, tel√©fono',
                'criticidad': 'Media'
            },

            # 7Ô∏è‚É£ Cambio de titularidad
            "cambio_titularidad": {
                'nombre': 'Cambio de titularidad',
                'descripcion': 'Solicitudes o errores en cambio de titular (con o sin subrogaci√≥n)',
                'criticidad': 'Media'
            },

            # 8Ô∏è‚É£ Ofertas y contrataci√≥n
            "ofertas_contratacion": {
                'nombre': 'Ofertas y contrataci√≥n',
                'descripcion': 'Creaci√≥n, modificaci√≥n, validaci√≥n o firma de ofertas',
                'criticidad': 'Media'
            },

            # 9Ô∏è‚É£ Tarifas y productos
            "tarifas_productos": {
                'nombre': 'Tarifas y productos',
                'descripcion': 'Cambio o correcci√≥n de tarifas, productos contratados, servicios adicionales, IEH y precios',
                'criticidad': 'Media'
            },

            # üîü Gesti√≥n de contratos
            "gestion_contratos": {
                'nombre': 'Gesti√≥n de contratos',
                'descripcion': 'Modificaci√≥n de ofertas, actualizaci√≥n de contratos y cambios en condiciones contractuales',
                'criticidad': 'Media'
            },

            # 1Ô∏è‚É£1Ô∏è‚É£ Bono social y vulnerabilidad
            "bono_social_vulnerabilidad": {
                'nombre': 'Bono social y vulnerabilidad',
                'descripcion': 'Alta, renovaci√≥n, correcci√≥n de datos del bono social',
                'criticidad': 'Media'
            },

            # 1Ô∏è‚É£1Ô∏è‚É£ Rechazos y bloqueos
            "rechazos_bloqueos": {
                'nombre': 'Rechazos y bloqueos',
                'descripcion': 'Rechazos de solicitudes o bloqueos en proceso',
                'criticidad': 'Alta'
            },

            # 1Ô∏è‚É£2Ô∏è‚É£ Cobros y pagos
            "cobros_pagos": {
                'nombre': 'Cobros y pagos',
                'descripcion': 'Problemas en cobros, pagos, devoluciones, pases a fallido',
                'criticidad': 'Media'
            },

            # 1Ô∏è‚É£3Ô∏è‚É£ Batch/Procesos autom√°ticos
            "batch_procesos_automaticos": {
                'nombre': 'Batch/Procesos autom√°ticos',
                'descripcion': 'Errores t√©cnicos en procesos batch, FTP, interfaces',
                'criticidad': 'Alta'
            },

            # 1Ô∏è‚É£4Ô∏è‚É£ Extracciones e informes
            "extracciones_informes": {
                'nombre': 'Extracciones e informes',
                'descripcion': 'Solicitudes de listados, informes, queries',
                'criticidad': 'Baja'
            },

            # 1Ô∏è‚É£5Ô∏è‚É£ Telemedida y medici√≥n remota
            "telemedida_medicion_remota": {
                'nombre': 'Telemedida y medici√≥n remota',
                'descripcion': 'Indicador de telemedida, problemas en medici√≥n remota',
                'criticidad': 'Media'
            },

            # 1Ô∏è‚É£6Ô∏è‚É£ Errores XML/mensajer√≠a
            "errores_xml_mensajeria": {
                'nombre': 'Errores XML/mensajer√≠a',
                'descripcion': 'Errores en generaci√≥n o recepci√≥n de XMLs y mensajes',
                'criticidad': 'Alta'
            },

            # 1Ô∏è‚É£7Ô∏è‚É£ Integraciones externas
            "integraciones_externas": {
                'nombre': 'Integraciones externas',
                'descripcion': 'Problemas con sistemas externos (GNClick, Salesforce, Markets)',
                'criticidad': 'Alta'
            },

            # 1Ô∏è‚É£8Ô∏è‚É£ Campa√±as y marketing
            "campanas_marketing": {
                'nombre': 'Campa√±as y marketing',
                'descripcion': 'Ejecuci√≥n o errores en campa√±as',
                'criticidad': 'Baja'
            },

            # 1Ô∏è‚É£9Ô∏è‚É£ Plantillas y documentaci√≥n
            "plantillas_documentacion": {
                'nombre': 'Plantillas y documentaci√≥n',
                'descripcion': 'Subida, eliminaci√≥n o modificaci√≥n de plantillas y documentos',
                'criticidad': 'Baja'
            },

            # 2Ô∏è‚É£0Ô∏è‚É£ Consultas y soporte funcional
            "consultas_soporte_funcional": {
                'nombre': 'Consultas y soporte funcional',
                'descripcion': 'Preguntas sobre funcionamiento, tablas, procesos, criterios',
                'criticidad': 'Baja'
            },

            # 2Ô∏è‚É£1Ô∏è‚É£ Gesti√≥n de usuarios
            "gestion_usuarios": {
                'nombre': 'Gesti√≥n de usuarios',
                'descripcion': 'Reasignaci√≥n, cambios de usuario, permisos y accesos del sistema',
                'criticidad': 'Media'
            },

            # 2Ô∏è‚É£2Ô∏è‚É£ Gestiones internas administrativas
            "gestiones_internas_administrativas": {
                'nombre': 'Gestiones internas administrativas',
                'descripcion': 'Tareas administrativas internas, creaci√≥n de puestos de trabajo, gestiones internas',
                'criticidad': 'Baja'
            },

            # 2Ô∏è‚É£3Ô∏è‚É£ Gesti√≥n de ofertas
            "gestion_ofertas": {
                'nombre': 'Gesti√≥n de ofertas',
                'descripcion': 'Activaci√≥n, modificaci√≥n y gesti√≥n de ofertas comerciales, l√≠neas de oferta y par√°metros',
                'criticidad': 'Media'
            },

            # 2Ô∏è‚É£4Ô∏è‚É£ Sincronizaci√≥n de datos
            "sincronizacion_datos": {
                'nombre': 'Sincronizaci√≥n de datos',
                'descripcion': 'Problemas de replicaci√≥n, sincronizaci√≥n y actualizaci√≥n de datos entre sistemas',
                'criticidad': 'Media'
            }
        }
        
        # Calcular puntuaciones para cada categor√≠a
        category_scores = {}
        matched_keywords = {}
        
        for category, keyword_list in keywords.items():
            score = 0
            matches = []
            for keyword in keyword_list:
                if keyword.lower() in combined_text:
                    score += 1
                    matches.append(keyword)
            
            if score > 0:
                category_scores[category] = score
                matched_keywords[category] = matches
        
        # Si hay coincidencias, seleccionar la categor√≠a con mayor puntuaci√≥n
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            best_score = category_scores[best_category]
            
            # Calcular confianza basada en n√∫mero de coincidencias
            confidence = min(0.95, 0.70 + (best_score * 0.05))
            
            # üéØ CONTROL DE CONFIANZA M√çNIMA - Si confianza < 0.70, categorizar como "Sin determinar"
            if confidence < 0.70:
                return {
                    'predicted_type': 'sin_determinar',
                    'confidence': confidence,
                    'type_info': {
                        'nombre': 'Sin determinar',
                        'descripcion': f'Confianza insuficiente ({confidence:.2f}) para clasificaci√≥n autom√°tica. Requiere revisi√≥n manual.',
                        'palabras_clave': ['baja confianza'] + matched_keywords[best_category][:2],
                        'nivel_criticidad': 'No evaluada'
                    },
                    'extracted_entities': {},
                    'processed_text': combined_text,
                    'rule_matched': f'Confianza baja: {confidence:.2f}',
                    'score': best_score,
                    'original_category': best_category
                }
            
            category_data = category_info[best_category]
            matched_keys = matched_keywords[best_category][:3]  # Top 3 keywords
            
            return {
                'predicted_type': best_category,
                'confidence': confidence,
                'type_info': {
                    'nombre': category_data['nombre'],
                    'descripcion': category_data['descripcion'],
                    'palabras_clave': matched_keys + ['clasificaci√≥n autom√°tica'],
                    'nivel_criticidad': category_data['criticidad']
                },
                'extracted_entities': {},
                'processed_text': combined_text,
                'rule_matched': ', '.join(matched_keys[:2]),
                'score': best_score
            }
        
        return None
    
    def _evaluate_system(self, df: pd.DataFrame, cluster_results: Dict, 
                        model_results: Dict) -> Dict[str, float]:
        """Eval√∫a la performance del sistema"""
        return {
            'silhouette_score': cluster_results.get('silhouette_score', 0.0),
            'model_accuracy': model_results.get('test_accuracy', 0.0),
            'model_cv_score': model_results.get('cv_score', 0.0),
            'num_clusters': len(np.unique(cluster_results['labels'])),
            'coverage': len(df) / len(df)
        }


class TextPreprocessor:
    """Preprocesador de texto con reglas espec√≠ficas de Naturgy"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.setup_preprocessing_rules()
    
    def setup_preprocessing_rules(self):
        """Configura reglas de preprocesamiento"""
        
        # Stop words seguras (eliminar)
        self.stop_words_safe = [
            'buenos d√≠as', 'cordial saludo', 'gracias', 'un saludo', 'saludos',
            'muchas gracias', 'quedo atento', 'quedo atenta', 'favor', 'por favor',
            'adjunto', 'env√≠o', 'enviado', 'estimado', 'estimada', 'hola', 'buen d√≠a'
        ]
        
        # Sinonimias para normalizaci√≥n
        self.synonyms = {
            'fallo': 'error', 'incidencia': 'error', 'problema': 'error',
            'rechazo': 'error', 'cancelaci√≥n': 'baja', 'anulaci√≥n': 'baja',
            'activaci√≥n': 'alta', 'creaci√≥n': 'alta', 'cambiar': 'modificar',
            'actualizar': 'modificar', 'corregir': 'modificar'
        }
    
    def process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Procesa DataFrame completo"""
        df_processed = df.copy()
        
        # Combinar columnas de texto
        df_processed['combined_text'] = self._combine_text_columns(df_processed)
        
        # Limpiar y normalizar texto
        df_processed['processed_text'] = df_processed['combined_text'].apply(
            self._process_text
        )
        
        return df_processed
    
    def _combine_text_columns(self, df: pd.DataFrame) -> pd.Series:
        """Combina columnas de texto relevantes - MEJORADO: Incluye Notas con mayor peso"""
        # Orden de prioridad: Resumen tiene mayor peso, seguido de Notas
        text_columns = ['Resumen', 'Notas', 'Tipo de ticket', 'Resoluci√≥n']
        combined = []
        
        for _, row in df.iterrows():
            text_parts = []
            
            # Resumen: incluir siempre si existe (mayor peso)
            if 'Resumen' in df.columns and pd.notna(row['Resumen']):
                resumen = str(row['Resumen']).strip()
                if resumen:
                    text_parts.append(resumen)
            
            # Notas: incluir con peso importante (segunda prioridad)
            if 'Notas' in df.columns and pd.notna(row['Notas']):
                notas = str(row['Notas']).strip()
                if notas and notas.lower() not in ['nan', '', 'null']:
                    text_parts.append(notas)
            
            # Otras columnas con menor peso
            for col in ['Tipo de ticket', 'Resoluci√≥n']:
                if col in df.columns and pd.notna(row[col]):
                    text_value = str(row[col]).strip()
                    if text_value and text_value.lower() not in ['nan', '', 'null']:
                        text_parts.append(text_value)
            
            combined.append(' '.join(text_parts))
        
        return pd.Series(combined)
    
    def _process_text(self, text: str) -> str:
        """Procesa un texto individual"""
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text).lower()
        
        # Limpiar caracteres especiales pero mantener espacios y n√∫meros
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # Eliminar stop words seguras
        for stop_word in self.stop_words_safe:
            text = text.replace(stop_word.lower(), ' ')
        
        # Aplicar sinonimias
        for synonym, standard in self.synonyms.items():
            text = text.replace(synonym.lower(), standard.lower())
        
        # Limpiar espacios extras
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text


class EntityExtractor:
    """Extractor de entidades espec√≠ficas del dominio"""
    
    def __init__(self):
        self.setup_patterns()
    
    def setup_patterns(self):
        """Configura patrones de extracci√≥n"""
        self.patterns = {
            'cups': r'ES\d{4}\d{4}\d{4}\d{4}[A-Z]{2}\d[A-Z]',
            'sr': r'R\d{2}-\d+',
            'req': r'REQ\d+',
            'ofl': r'OFL\d+',
            'fechas': r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}',
            'productos': r'\b(bono social|cuidahogar|rl1|tur vulnerable)\b',
            'estados': r'\b(activo|inactivo|pendiente|bloqueado|calculable)\b'
        }
    
    def extract_entities(self, df: pd.DataFrame) -> pd.DataFrame:
        """Extrae entidades de todos los textos"""
        df_processed = df.copy()
        
        entities_list = []
        for text in df_processed['combined_text']:
            entities = self._extract_from_text(text)
            entities_list.append(entities)
        
        df_processed['entities'] = entities_list
        
        # Crear columnas individuales para cada tipo de entidad
        for entity_type in self.patterns.keys():
            df_processed[f'has_{entity_type}'] = [
                len(entities.get(entity_type, [])) > 0 
                for entities in entities_list
            ]
        
        return df_processed
    
    def _extract_from_text(self, text: str) -> Dict[str, List[str]]:
        """Extrae entidades de un texto"""
        if pd.isna(text):
            return {entity_type: [] for entity_type in self.patterns.keys()}
        
        text = str(text).lower()
        entities = {}
        
        for entity_type, pattern in self.patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            entities[entity_type] = list(set(matches))
        
        return entities


class IncidentClusterer:
    """Sistema de clustering para agrupar incidencias similares"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.vectorizer = None
        self.clustering_model = None
    
    def cluster_incidents(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Realiza clustering de incidencias"""
        print("üéØ Iniciando clustering de incidencias...")
        
        # Vectorizar textos
        X = self._vectorize_texts(df['processed_text'])
        
        # Determinar n√∫mero √≥ptimo de clusters
        optimal_k = self._find_optimal_clusters(X)
        
        # Realizar clustering
        labels = self._perform_clustering(X, optimal_k)
        
        # Evaluar clustering
        metrics = self._evaluate_clustering(X, labels)
        
        print(f"‚úÖ Clustering completado: {optimal_k} clusters identificados")
        
        return {
            'labels': labels,
            'n_clusters': optimal_k,
            'vectorizer': self.vectorizer,
            'silhouette_score': metrics.get('silhouette_score', 0.0),
            'cluster_sizes': pd.Series(labels).value_counts().to_dict()
        }
    
    def _vectorize_texts(self, texts: pd.Series) -> np.ndarray:
        """Vectoriza textos usando TF-IDF"""
        self.vectorizer = TfidfVectorizer(
            max_features=self.config['tfidf_max_features'],
            min_df=self.config['tfidf_min_df'],
            max_df=self.config['tfidf_max_df'],
            ngram_range=(1, 2),
            stop_words=None
        )
        
        X = self.vectorizer.fit_transform(texts.fillna(''))
        print(f"üìä Vectorizaci√≥n completada: {X.shape}")
        return X
    
    def _find_optimal_clusters(self, X: np.ndarray) -> int:
        """Encuentra el n√∫mero √≥ptimo de clusters"""
        max_k = min(self.config['max_clusters'], X.shape[0] // self.config['min_cluster_size'])
        
        if max_k < 2:
            return 2
        
        print(f"üîç Evaluando clustering desde 2 hasta {max_k} clusters...")
        
        from sklearn.metrics import silhouette_score
        
        inertias = []
        silhouette_scores = []
        k_range = range(2, min(max_k + 1, 30))
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=self.config['random_state'], n_init=10)
            labels = kmeans.fit_predict(X)
            inertias.append(kmeans.inertia_)
            
            try:
                sil_score = silhouette_score(X, labels)
                silhouette_scores.append(sil_score)
            except:
                silhouette_scores.append(0.0)
        
        # Seleccionar basado en tama√±o del dataset para maximizar diversidad
        if X.shape[0] > 3000:
            optimal_k = min(25, max_k)
        elif X.shape[0] > 1000:
            optimal_k = min(15, max_k)
        else:
            optimal_k = min(10, max_k)
        
        print(f"üéØ N√∫mero √≥ptimo de clusters seleccionado: {optimal_k}")
        return optimal_k
    
    def _perform_clustering(self, X: np.ndarray, n_clusters: int) -> np.ndarray:
        """Realiza el clustering"""
        self.clustering_model = KMeans(
            n_clusters=n_clusters,
            random_state=self.config['random_state'],
            n_init=10,
            max_iter=300
        )
        
        labels = self.clustering_model.fit_predict(X)
        return labels
    
    def _evaluate_clustering(self, X: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """Eval√∫a la calidad del clustering"""
        from sklearn.metrics import silhouette_score
        
        try:
            if len(np.unique(labels)) > 1:
                sil_score = silhouette_score(X, labels)
            else:
                sil_score = 0.0
        except:
            sil_score = 0.0
        
        return {
            'silhouette_score': sil_score,
            'n_clusters': len(np.unique(labels)),
            'largest_cluster_size': np.max(np.bincount(labels))
        }


class PredictiveClassifier:
    """Modelo predictivo para clasificaci√≥n autom√°tica"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.model = None
        self.feature_vectorizer = None
        self.label_encoder = None
        self.is_trained = False
    
    def train_model(self, df: pd.DataFrame, cluster_labels: np.ndarray) -> Dict[str, Any]:
        """Entrena el modelo predictivo"""
        print("ü§ñ Entrenando modelo predictivo...")
        
        # Preparar features
        X = self._prepare_features(df)
        y = cluster_labels
        
        # Codificar etiquetas
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        
        # Dividir datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.2, random_state=self.config['random_state'],
            stratify=y_encoded if len(np.unique(y_encoded)) > 1 else None
        )
        
        # Entrenar modelo
        self.model = RandomForestClassifier(
            n_estimators=100,
            random_state=self.config['random_state'],
            n_jobs=-1
        )
        self.model.fit(X_train, y_train)
        
        # Evaluar modelo
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)
        
        self.is_trained = True
        
        print(f"‚úÖ Modelo entrenado - Accuracy: {test_score:.3f}")
        
        return {
            'train_accuracy': train_score,
            'test_accuracy': test_score,
            'cv_score': test_score,
            'model': self.model
        }
    
    def predict(self, df: pd.DataFrame) -> Tuple[str, float]:
        """Predice la clasificaci√≥n de nuevas incidencias"""
        if not self.is_trained:
            raise ValueError("El modelo debe ser entrenado primero")
        
        X = self._prepare_features(df)
        
        # Predicci√≥n
        prediction = self.model.predict(X)[0]
        
        # Calcular confianza
        try:
            if hasattr(self.model, 'predict_proba'):
                probabilities = self.model.predict_proba(X)[0]
                confidence = np.max(probabilities)
            else:
                confidence = 0.8
        except:
            confidence = 0.5
        
        # Decodificar etiqueta
        original_label = self.label_encoder.inverse_transform([prediction])[0]
        
        return f"tipo_{original_label:02d}", confidence
    
    def _prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """Prepara features para el modelo"""
        if self.feature_vectorizer is None:
            self.feature_vectorizer = TfidfVectorizer(
                max_features=self.config['tfidf_max_features'],
                min_df=self.config['tfidf_min_df'],
                max_df=self.config['tfidf_max_df'],
                ngram_range=(1, 2)
            )
            X_text = self.feature_vectorizer.fit_transform(df['processed_text'].fillna(''))
        else:
            X_text = self.feature_vectorizer.transform(df['processed_text'].fillna(''))
        
        return X_text


# Utilidades para ejecutar el pipeline completo
class IncidentAnalysisRunner:
    """Clase utilitaria para ejecutar an√°lisis completos"""
    
    @staticmethod
    def run_complete_analysis(data_path: str, output_dir: str = 'outputs') -> None:
        """Ejecuta an√°lisis completo y genera reportes"""
        print("üöÄ Iniciando an√°lisis completo de incidencias Naturgy...")
        
        # Crear clasificador con directorio de salida personalizado
        classifier = NaturgyIncidentClassifier(output_dir=output_dir)
        
        # Entrenar pipeline
        results = classifier.train_pipeline(data_path)
        
        print("‚úÖ An√°lisis completo finalizado!")
        print(f"üìÇ Resultados organizados en: {output_dir}/")
        print("üìä Estructura creada:")
        print(f"  - {output_dir}/models/ (modelos entrenados)")
        print(f"  - {output_dir}/data/ (datos JSON)")
        print(f"  - {output_dir}/reports/ (reportes de texto)")
        print(f"  - {output_dir}/logs/ (archivos de registro)")


# Script principal para ejecutar desde l√≠nea de comandos
if __name__ == "__main__":
    import sys
    
    def main():
        """Funci√≥n principal para ejecutar el pipeline"""
        print("üöÄ NATURGY AI INCIDENT CLASSIFIER - VERSI√ìN REFACTORIZADA")
        print("=" * 65)
        
        if len(sys.argv) < 2:
            print("Uso: python naturgy_classifier_refactored.py <archivo_datos.xlsx> [directorio_salida]")
            print("\nEjemplo:")
            print("  python naturgy_classifier_refactored.py infomation.xlsx ./outputs_nuevos")
            sys.exit(1)
        
        data_path = sys.argv[1]
        output_dir = sys.argv[2] if len(sys.argv) > 2 else 'outputs'
        
        try:
            # Ejecutar an√°lisis completo
            IncidentAnalysisRunner.run_complete_analysis(data_path, output_dir)
            
            print("\n" + "=" * 65)
            print("‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE")
            print(f"üìÇ Resultados guardados en: {output_dir}")
            print("üìä Mejoras implementadas:")
            print("  ‚úì Estructura de carpetas organizada")
            print("  ‚úì Nomenclatura sem√°nticamente coherente")
            print("  ‚úì Reportes categorizados por tipo")
            print("  ‚úì An√°lisis de criticidad incluido")
            
        except Exception as e:
            print(f"‚ùå Error durante el an√°lisis: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    main()
